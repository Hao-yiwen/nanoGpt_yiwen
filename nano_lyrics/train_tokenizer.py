# -*- coding: utf-8 -*-
"""
å•ç‹¬è®­ç»ƒ Tokenizer è„šæœ¬

è®­ç»ƒ BPE tokenizer å¹¶ç”Ÿæˆ HuggingFace Hub å…¼å®¹æ–‡ä»¶
"""

import json
from config import LYRICS_DIR, MAX_SONGS, BPE_VOCAB_SIZE
from tokenizer import train_bpe_tokenizer, encode, decode, get_vocab_size
import re


def load_lyrics(max_songs=MAX_SONGS):
    """åŠ è½½æ­Œè¯æ–‡ä»¶"""
    all_lyrics = []
    for i in range(1, 6):
        if len(all_lyrics) >= max_songs:
            break
        filepath = f'{LYRICS_DIR}/lyrics{i}.json'
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                songs = json.load(f)
                remaining = max_songs - len(all_lyrics)
                all_lyrics.extend(songs[:remaining])
        except FileNotFoundError:
            print(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
            continue
    return all_lyrics


def preprocess_lyric(lyric_lines):
    """é¢„å¤„ç†æ­Œè¯ï¼Œè¿‡æ»¤å…ƒä¿¡æ¯"""
    skip_patterns = [
        r'^ä½œè¯', r'^ä½œæ›²', r'^ç¼–æ›²', r'^æ¼”å”±', r'^å’Œå£°', r'^åæœŸ', r'^æ··éŸ³',
        r'^åˆ¶ä½œ', r'^å½•éŸ³', r'^å‰ä»–', r'^é’¢ç´', r'^è´æ–¯', r'^é¼“',
        r'^ä¸»æ­Œ\d*$', r'^å‰¯æ­Œ\d*$', r'^è¿‡æ¸¡\d*$', r'^é—´å¥', r'^ç»“å°¾',
        r'^verse', r'^chorus', r'^bridge', r'^\[', r'^ã€', r'^ï¼ˆ.*ï¼‰$',
        r'^\(.*\)$', r'^demo$', r'^intro', r'^outro',
    ]

    result = []
    for line in lyric_lines:
        line = line.strip()
        if not line:
            continue
        skip = any(re.match(p, line, re.IGNORECASE) for p in skip_patterns)
        if not skip and len(line) > 1:
            result.append(line)

    return '\n'.join(result)


def main():
    print("=" * 50)
    print("è®­ç»ƒ BPE Tokenizer")
    print("=" * 50)

    # åŠ è½½æ­Œè¯
    print("\næ­£åœ¨åŠ è½½æ­Œè¯...")
    lyrics_data = load_lyrics()
    print(f"åŠ è½½äº† {len(lyrics_data)} é¦–æ­Œæ›²")

    # é¢„å¤„ç†
    processed_lyrics = [
        preprocess_lyric(song['lyric'])
        for song in lyrics_data
        if song.get('lyric')
    ]
    processed_lyrics = [lyric for lyric in processed_lyrics if lyric.strip()]

    # åˆå¹¶æ–‡æœ¬
    text = '\n\n'.join(processed_lyrics)
    print(f"æœ‰æ•ˆæ­Œæ›²æ•°: {len(processed_lyrics):,}")
    print(f"æ€»å­—ç¬¦æ•°: {len(text):,}")
    print(f"å”¯ä¸€å­—ç¬¦æ•°: {len(set(text)):,}")

    # è®­ç»ƒ tokenizer
    print(f"\nè®­ç»ƒ BPE tokenizer (vocab_size={BPE_VOCAB_SIZE})...")
    tokenizer = train_bpe_tokenizer(text)

    # æ˜¾ç¤ºç»“æœ
    print(f"\nè¯æ±‡è¡¨å¤§å°: {get_vocab_size(tokenizer)}")

    # æµ‹è¯•
    test_texts = ["ä»Šå¤œæˆ‘", "çˆ±æƒ…", "Hello World", "ğŸµ éŸ³ä¹"]
    print("\nåˆ†è¯æµ‹è¯•:")
    for t in test_texts:
        encoded = encode(t, tokenizer)
        decoded = decode(encoded, tokenizer)
        print(f"  '{t}' -> {encoded} -> '{decoded}'")

    print("\n" + "=" * 50)
    print("è®­ç»ƒå®Œæˆï¼")
    print("Hub æ–‡ä»¶ä¿å­˜åœ¨: chinese_lyrics_bpe/")
    print("=" * 50)


if __name__ == '__main__':
    main()
