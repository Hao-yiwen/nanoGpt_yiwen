{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc9773ac",
   "metadata": {},
   "source": [
    "# 训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ffa85c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-01 22:03:16--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.1’\n",
      "\n",
      "input.txt.1         100%[===================>]   1.06M  1.06MB/s    in 1.0s    \n",
      "\n",
      "2025-12-01 22:03:18 (1.06 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b5faf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77311a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters:\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb765aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2935502f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c16bc9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "# 这里，我们为字符集中的每个字符分配一个唯一的数字ID\n",
    "stoi = { ch:i for i,ch in enumerate(chars)}    # 字符到索引（string to index）的映射\n",
    "itos = { i:ch for i,ch in enumerate(chars)}    # 索引到字符（index to string）的映射\n",
    "\n",
    "# 定义一个编码函数，将字符串转换为数字ID序列，方便神经网络处理\n",
    "encodec = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encodec(\"hello\"))\n",
    "print(decode(encodec(\"hello\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8bc131d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encodec(text), dtype=torch.long)\n",
    "\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41c5863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6e2d43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45818b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is 47\n",
      "when input is tensor([18, 47]) the target is 56\n",
      "when input is tensor([18, 47, 56]) the target is 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a423c278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) \n",
      " torch.Size([4, 8])\n",
      "when input is tensor([24]) the target is 43\n",
      "when input is tensor([24, 43]) the target is 58\n",
      "when input is tensor([24, 43, 58]) the target is 5\n",
      "when input is tensor([24, 43, 58,  5]) the target is 57\n",
      "when input is tensor([24, 43, 58,  5, 57]) the target is 1\n",
      "when input is tensor([24, 43, 58,  5, 57,  1]) the target is 46\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46]) the target is 43\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46, 43]) the target is 39\n",
      "when input is tensor([44]) the target is 53\n",
      "when input is tensor([44, 53]) the target is 56\n",
      "when input is tensor([44, 53, 56]) the target is 1\n",
      "when input is tensor([44, 53, 56,  1]) the target is 58\n",
      "when input is tensor([44, 53, 56,  1, 58]) the target is 46\n",
      "when input is tensor([44, 53, 56,  1, 58, 46]) the target is 39\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39]) the target is 58\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39, 58]) the target is 1\n",
      "when input is tensor([52]) the target is 58\n",
      "when input is tensor([52, 58]) the target is 1\n",
      "when input is tensor([52, 58,  1]) the target is 58\n",
      "when input is tensor([52, 58,  1, 58]) the target is 46\n",
      "when input is tensor([52, 58,  1, 58, 46]) the target is 39\n",
      "when input is tensor([52, 58,  1, 58, 46, 39]) the target is 58\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58]) the target is 1\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58,  1]) the target is 46\n",
      "when input is tensor([25]) the target is 17\n",
      "when input is tensor([25, 17]) the target is 27\n",
      "when input is tensor([25, 17, 27]) the target is 10\n",
      "when input is tensor([25, 17, 27, 10]) the target is 0\n",
      "when input is tensor([25, 17, 27, 10,  0]) the target is 21\n",
      "when input is tensor([25, 17, 27, 10,  0, 21]) the target is 1\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1]) the target is 54\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1, 54]) the target is 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else test_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(xb.shape, '\\n', yb.shape)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context} the target is {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcae5128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6cdc5514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "torch.Size([32, 65])\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# 定义一个“大ram语言模型”类，继承自nn.Module，表示一个能基于前一个词预测下一个词的神经网络模型\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.token_embeding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embeding_table(idx)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html\n",
    "            \n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "        \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # self(idx) 的作用是调用模型的 forward 方法，等价于 self.forward(idx)\n",
    "            # 它返回 (logits, loss)，这里只关心 logits 部分\n",
    "            logits, _ = self(idx)\n",
    "            # logits的返回是在BigramLanguageModel.forward方法里，\n",
    "            # 那里(logits, loss)返回的logits已经是view过后的形状 (B*T, C)。\n",
    "            # 所以这里logits并不是 (B, T, C)，而是 (B*T, C)\n",
    "            # 如果真的要取每个batch的最后一个时间步的logit，需要先把logits reshape 回 (B, T, C)\n",
    "            B, T = idx.shape\n",
    "            logits = logits.view(B, T, -1)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # softmax 返回的是所有类别的概率分布，而不是最大概率的索引\n",
    "            # 如果想要最大索引（即贪婪采样），应该用 argmax\n",
    "            # 用 torch.multinomial 是为了从概率分布中按概率随机采样，增加生成多样性\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(loss)\n",
    "print(logits.shape)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45cb3d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "516d5c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss=2.4675867557525635\n",
      "step 1: loss=2.5339956283569336\n",
      "step 2: loss=2.621551036834717\n",
      "step 3: loss=2.448259115219116\n",
      "step 4: loss=2.480936288833618\n",
      "step 5: loss=2.5009267330169678\n",
      "step 6: loss=2.380303144454956\n",
      "step 7: loss=2.499534845352173\n",
      "step 8: loss=2.4553537368774414\n",
      "step 9: loss=2.5483171939849854\n",
      "step 10: loss=2.500459671020508\n",
      "step 11: loss=2.3634114265441895\n",
      "step 12: loss=2.443803071975708\n",
      "step 13: loss=2.4735116958618164\n",
      "step 14: loss=2.4223384857177734\n",
      "step 15: loss=2.626405715942383\n",
      "step 16: loss=2.559792995452881\n",
      "step 17: loss=2.4779586791992188\n",
      "step 18: loss=2.5188212394714355\n",
      "step 19: loss=2.6077866554260254\n",
      "step 20: loss=2.630239248275757\n",
      "step 21: loss=2.3523497581481934\n",
      "step 22: loss=2.591978073120117\n",
      "step 23: loss=2.458550214767456\n",
      "step 24: loss=2.4922313690185547\n",
      "step 25: loss=2.4848382472991943\n",
      "step 26: loss=2.5086498260498047\n",
      "step 27: loss=2.428760290145874\n",
      "step 28: loss=2.476320266723633\n",
      "step 29: loss=2.4554929733276367\n",
      "step 30: loss=2.4172627925872803\n",
      "step 31: loss=2.418762445449829\n",
      "step 32: loss=2.527923822402954\n",
      "step 33: loss=2.546017646789551\n",
      "step 34: loss=2.3937313556671143\n",
      "step 35: loss=2.458036422729492\n",
      "step 36: loss=2.3937485218048096\n",
      "step 37: loss=2.3499715328216553\n",
      "step 38: loss=2.396587371826172\n",
      "step 39: loss=2.3999276161193848\n",
      "step 40: loss=2.511756181716919\n",
      "step 41: loss=2.6544482707977295\n",
      "step 42: loss=2.3614652156829834\n",
      "step 43: loss=2.519726514816284\n",
      "step 44: loss=2.5252490043640137\n",
      "step 45: loss=2.417133092880249\n",
      "step 46: loss=2.530493974685669\n",
      "step 47: loss=2.519987106323242\n",
      "step 48: loss=2.6098134517669678\n",
      "step 49: loss=2.360199451446533\n",
      "step 50: loss=2.366025686264038\n",
      "step 51: loss=2.510911226272583\n",
      "step 52: loss=2.6730215549468994\n",
      "step 53: loss=2.4747111797332764\n",
      "step 54: loss=2.5153961181640625\n",
      "step 55: loss=2.490746021270752\n",
      "step 56: loss=2.5325112342834473\n",
      "step 57: loss=2.6179797649383545\n",
      "step 58: loss=2.3639883995056152\n",
      "step 59: loss=2.487668037414551\n",
      "step 60: loss=2.4752297401428223\n",
      "step 61: loss=2.585014581680298\n",
      "step 62: loss=2.3850622177124023\n",
      "step 63: loss=2.408672571182251\n",
      "step 64: loss=2.5122482776641846\n",
      "step 65: loss=2.4974074363708496\n",
      "step 66: loss=2.331256628036499\n",
      "step 67: loss=2.4960196018218994\n",
      "step 68: loss=2.394940137863159\n",
      "step 69: loss=2.503499746322632\n",
      "step 70: loss=2.494401216506958\n",
      "step 71: loss=2.371856451034546\n",
      "step 72: loss=2.5335605144500732\n",
      "step 73: loss=2.438337802886963\n",
      "step 74: loss=2.5413942337036133\n",
      "step 75: loss=2.633147954940796\n",
      "step 76: loss=2.4502735137939453\n",
      "step 77: loss=2.5188755989074707\n",
      "step 78: loss=2.477382183074951\n",
      "step 79: loss=2.459779739379883\n",
      "step 80: loss=2.46933913230896\n",
      "step 81: loss=2.595761299133301\n",
      "step 82: loss=2.457348108291626\n",
      "step 83: loss=2.4076128005981445\n",
      "step 84: loss=2.5224668979644775\n",
      "step 85: loss=2.417092800140381\n",
      "step 86: loss=2.404066801071167\n",
      "step 87: loss=2.4984402656555176\n",
      "step 88: loss=2.582709550857544\n",
      "step 89: loss=2.3176374435424805\n",
      "step 90: loss=2.5126497745513916\n",
      "step 91: loss=2.4605419635772705\n",
      "step 92: loss=2.479367733001709\n",
      "step 93: loss=2.466919183731079\n",
      "step 94: loss=2.545577049255371\n",
      "step 95: loss=2.4008708000183105\n",
      "step 96: loss=2.5046188831329346\n",
      "step 97: loss=2.4642133712768555\n",
      "step 98: loss=2.449965476989746\n",
      "step 99: loss=2.471540927886963\n",
      "step 100: loss=2.5351293087005615\n",
      "step 101: loss=2.44116473197937\n",
      "step 102: loss=2.5016534328460693\n",
      "step 103: loss=2.3580574989318848\n",
      "step 104: loss=2.463747262954712\n",
      "step 105: loss=2.4462978839874268\n",
      "step 106: loss=2.4429945945739746\n",
      "step 107: loss=2.509068012237549\n",
      "step 108: loss=2.555732488632202\n",
      "step 109: loss=2.4790709018707275\n",
      "step 110: loss=2.458698272705078\n",
      "step 111: loss=2.4865095615386963\n",
      "step 112: loss=2.468716859817505\n",
      "step 113: loss=2.4895260334014893\n",
      "step 114: loss=2.4834935665130615\n",
      "step 115: loss=2.4693799018859863\n",
      "step 116: loss=2.32470703125\n",
      "step 117: loss=2.411943197250366\n",
      "step 118: loss=2.580749273300171\n",
      "step 119: loss=2.5895345211029053\n",
      "step 120: loss=2.406343936920166\n",
      "step 121: loss=2.5714480876922607\n",
      "step 122: loss=2.3873088359832764\n",
      "step 123: loss=2.5395121574401855\n",
      "step 124: loss=2.455479145050049\n",
      "step 125: loss=2.420659303665161\n",
      "step 126: loss=2.4726123809814453\n",
      "step 127: loss=2.448488235473633\n",
      "step 128: loss=2.4609405994415283\n",
      "step 129: loss=2.5264673233032227\n",
      "step 130: loss=2.591094493865967\n",
      "step 131: loss=2.453354835510254\n",
      "step 132: loss=2.570374011993408\n",
      "step 133: loss=2.4901247024536133\n",
      "step 134: loss=2.4340152740478516\n",
      "step 135: loss=2.4501633644104004\n",
      "step 136: loss=2.54483699798584\n",
      "step 137: loss=2.3856823444366455\n",
      "step 138: loss=2.4685299396514893\n",
      "step 139: loss=2.51346755027771\n",
      "step 140: loss=2.473517656326294\n",
      "step 141: loss=2.478560209274292\n",
      "step 142: loss=2.53212571144104\n",
      "step 143: loss=2.473471164703369\n",
      "step 144: loss=2.4277355670928955\n",
      "step 145: loss=2.4689536094665527\n",
      "step 146: loss=2.5207042694091797\n",
      "step 147: loss=2.4918978214263916\n",
      "step 148: loss=2.4116673469543457\n",
      "step 149: loss=2.563317060470581\n",
      "step 150: loss=2.491577625274658\n",
      "step 151: loss=2.48856520652771\n",
      "step 152: loss=2.4675371646881104\n",
      "step 153: loss=2.4272656440734863\n",
      "step 154: loss=2.5249102115631104\n",
      "step 155: loss=2.5030910968780518\n",
      "step 156: loss=2.414552688598633\n",
      "step 157: loss=2.5236330032348633\n",
      "step 158: loss=2.5202882289886475\n",
      "step 159: loss=2.4117681980133057\n",
      "step 160: loss=2.4104771614074707\n",
      "step 161: loss=2.586681604385376\n",
      "step 162: loss=2.5156259536743164\n",
      "step 163: loss=2.4060168266296387\n",
      "step 164: loss=2.510063886642456\n",
      "step 165: loss=2.434356927871704\n",
      "step 166: loss=2.5878491401672363\n",
      "step 167: loss=2.6080355644226074\n",
      "step 168: loss=2.4971811771392822\n",
      "step 169: loss=2.5193169116973877\n",
      "step 170: loss=2.435171365737915\n",
      "step 171: loss=2.471757173538208\n",
      "step 172: loss=2.3838164806365967\n",
      "step 173: loss=2.509707450866699\n",
      "step 174: loss=2.4190614223480225\n",
      "step 175: loss=2.435420036315918\n",
      "step 176: loss=2.4717330932617188\n",
      "step 177: loss=2.427441120147705\n",
      "step 178: loss=2.5584068298339844\n",
      "step 179: loss=2.683612585067749\n",
      "step 180: loss=2.4284303188323975\n",
      "step 181: loss=2.644346237182617\n",
      "step 182: loss=2.4633381366729736\n",
      "step 183: loss=2.447174072265625\n",
      "step 184: loss=2.580044746398926\n",
      "step 185: loss=2.597346544265747\n",
      "step 186: loss=2.547969102859497\n",
      "step 187: loss=2.4776787757873535\n",
      "step 188: loss=2.4660325050354004\n",
      "step 189: loss=2.5162277221679688\n",
      "step 190: loss=2.4335193634033203\n",
      "step 191: loss=2.3911192417144775\n",
      "step 192: loss=2.5094387531280518\n",
      "step 193: loss=2.5424249172210693\n",
      "step 194: loss=2.4790334701538086\n",
      "step 195: loss=2.523806571960449\n",
      "step 196: loss=2.5121421813964844\n",
      "step 197: loss=2.6674931049346924\n",
      "step 198: loss=2.4999403953552246\n",
      "step 199: loss=2.5712170600891113\n",
      "step 200: loss=2.3684194087982178\n",
      "step 201: loss=2.542600631713867\n",
      "step 202: loss=2.5410311222076416\n",
      "step 203: loss=2.41271710395813\n",
      "step 204: loss=2.4253947734832764\n",
      "step 205: loss=2.5647149085998535\n",
      "step 206: loss=2.4746294021606445\n",
      "step 207: loss=2.4119925498962402\n",
      "step 208: loss=2.5357258319854736\n",
      "step 209: loss=2.4362688064575195\n",
      "step 210: loss=2.356633424758911\n",
      "step 211: loss=2.5066020488739014\n",
      "step 212: loss=2.5165047645568848\n",
      "step 213: loss=2.4630465507507324\n",
      "step 214: loss=2.419555425643921\n",
      "step 215: loss=2.4727132320404053\n",
      "step 216: loss=2.543076753616333\n",
      "step 217: loss=2.4122936725616455\n",
      "step 218: loss=2.428180456161499\n",
      "step 219: loss=2.600395917892456\n",
      "step 220: loss=2.5611703395843506\n",
      "step 221: loss=2.553095817565918\n",
      "step 222: loss=2.4070487022399902\n",
      "step 223: loss=2.398831844329834\n",
      "step 224: loss=2.6601920127868652\n",
      "step 225: loss=2.42627215385437\n",
      "step 226: loss=2.494736433029175\n",
      "step 227: loss=2.456986427307129\n",
      "step 228: loss=2.5159475803375244\n",
      "step 229: loss=2.6145477294921875\n",
      "step 230: loss=2.395592212677002\n",
      "step 231: loss=2.5223095417022705\n",
      "step 232: loss=2.4714102745056152\n",
      "step 233: loss=2.442601203918457\n",
      "step 234: loss=2.4034717082977295\n",
      "step 235: loss=2.515429735183716\n",
      "step 236: loss=2.3908138275146484\n",
      "step 237: loss=2.598971366882324\n",
      "step 238: loss=2.427753210067749\n",
      "step 239: loss=2.619443893432617\n",
      "step 240: loss=2.512791395187378\n",
      "step 241: loss=2.4430155754089355\n",
      "step 242: loss=2.51289963722229\n",
      "step 243: loss=2.4254167079925537\n",
      "step 244: loss=2.531282663345337\n",
      "step 245: loss=2.3700852394104004\n",
      "step 246: loss=2.4888651371002197\n",
      "step 247: loss=2.509913921356201\n",
      "step 248: loss=2.466052293777466\n",
      "step 249: loss=2.485593557357788\n",
      "step 250: loss=2.3720219135284424\n",
      "step 251: loss=2.4779129028320312\n",
      "step 252: loss=2.3782501220703125\n",
      "step 253: loss=2.4900195598602295\n",
      "step 254: loss=2.3358521461486816\n",
      "step 255: loss=2.338641405105591\n",
      "step 256: loss=2.4436237812042236\n",
      "step 257: loss=2.5055155754089355\n",
      "step 258: loss=2.481157064437866\n",
      "step 259: loss=2.588047981262207\n",
      "step 260: loss=2.419830560684204\n",
      "step 261: loss=2.431288242340088\n",
      "step 262: loss=2.3949925899505615\n",
      "step 263: loss=2.365865707397461\n",
      "step 264: loss=2.4405221939086914\n",
      "step 265: loss=2.5435538291931152\n",
      "step 266: loss=2.5115480422973633\n",
      "step 267: loss=2.4424335956573486\n",
      "step 268: loss=2.52152156829834\n",
      "step 269: loss=2.4648892879486084\n",
      "step 270: loss=2.468266725540161\n",
      "step 271: loss=2.471745252609253\n",
      "step 272: loss=2.521832227706909\n",
      "step 273: loss=2.466585159301758\n",
      "step 274: loss=2.370098352432251\n",
      "step 275: loss=2.443079948425293\n",
      "step 276: loss=2.572190046310425\n",
      "step 277: loss=2.6317646503448486\n",
      "step 278: loss=2.4331960678100586\n",
      "step 279: loss=2.5257041454315186\n",
      "step 280: loss=2.419975519180298\n",
      "step 281: loss=2.469604969024658\n",
      "step 282: loss=2.4811668395996094\n",
      "step 283: loss=2.5287587642669678\n",
      "step 284: loss=2.5131425857543945\n",
      "step 285: loss=2.4982011318206787\n",
      "step 286: loss=2.5191376209259033\n",
      "step 287: loss=2.536137580871582\n",
      "step 288: loss=2.379037380218506\n",
      "step 289: loss=2.4823648929595947\n",
      "step 290: loss=2.5925850868225098\n",
      "step 291: loss=2.6680264472961426\n",
      "step 292: loss=2.3796064853668213\n",
      "step 293: loss=2.5507678985595703\n",
      "step 294: loss=2.4381134510040283\n",
      "step 295: loss=2.587791919708252\n",
      "step 296: loss=2.398097038269043\n",
      "step 297: loss=2.358022689819336\n",
      "step 298: loss=2.4946393966674805\n",
      "step 299: loss=2.3889193534851074\n",
      "step 300: loss=2.417296886444092\n",
      "step 301: loss=2.4726831912994385\n",
      "step 302: loss=2.5757689476013184\n",
      "step 303: loss=2.5483016967773438\n",
      "step 304: loss=2.5442116260528564\n",
      "step 305: loss=2.5525848865509033\n",
      "step 306: loss=2.521582841873169\n",
      "step 307: loss=2.2656028270721436\n",
      "step 308: loss=2.5032763481140137\n",
      "step 309: loss=2.4144744873046875\n",
      "step 310: loss=2.3896396160125732\n",
      "step 311: loss=2.422125816345215\n",
      "step 312: loss=2.53019642829895\n",
      "step 313: loss=2.558957576751709\n",
      "step 314: loss=2.5431063175201416\n",
      "step 315: loss=2.4873173236846924\n",
      "step 316: loss=2.5606517791748047\n",
      "step 317: loss=2.5002708435058594\n",
      "step 318: loss=2.439767599105835\n",
      "step 319: loss=2.471461772918701\n",
      "step 320: loss=2.4644479751586914\n",
      "step 321: loss=2.468672752380371\n",
      "step 322: loss=2.4884772300720215\n",
      "step 323: loss=2.4533674716949463\n",
      "step 324: loss=2.497678279876709\n",
      "step 325: loss=2.4480812549591064\n",
      "step 326: loss=2.526803731918335\n",
      "step 327: loss=2.478020429611206\n",
      "step 328: loss=2.5548818111419678\n",
      "step 329: loss=2.495037794113159\n",
      "step 330: loss=2.4476747512817383\n",
      "step 331: loss=2.4911630153656006\n",
      "step 332: loss=2.404463052749634\n",
      "step 333: loss=2.3985469341278076\n",
      "step 334: loss=2.4784200191497803\n",
      "step 335: loss=2.510697603225708\n",
      "step 336: loss=2.4128217697143555\n",
      "step 337: loss=2.432899236679077\n",
      "step 338: loss=2.581648349761963\n",
      "step 339: loss=2.413015604019165\n",
      "step 340: loss=2.527941942214966\n",
      "step 341: loss=2.397165536880493\n",
      "step 342: loss=2.442230701446533\n",
      "step 343: loss=2.420074224472046\n",
      "step 344: loss=2.520782947540283\n",
      "step 345: loss=2.5264852046966553\n",
      "step 346: loss=2.6799957752227783\n",
      "step 347: loss=2.6154215335845947\n",
      "step 348: loss=2.457921028137207\n",
      "step 349: loss=2.4703009128570557\n",
      "step 350: loss=2.384593963623047\n",
      "step 351: loss=2.5447998046875\n",
      "step 352: loss=2.500096321105957\n",
      "step 353: loss=2.4632840156555176\n",
      "step 354: loss=2.4048445224761963\n",
      "step 355: loss=2.531710147857666\n",
      "step 356: loss=2.5102505683898926\n",
      "step 357: loss=2.4488365650177\n",
      "step 358: loss=2.4307243824005127\n",
      "step 359: loss=2.50053334236145\n",
      "step 360: loss=2.636493444442749\n",
      "step 361: loss=2.4022953510284424\n",
      "step 362: loss=2.4009206295013428\n",
      "step 363: loss=2.4644124507904053\n",
      "step 364: loss=2.4969849586486816\n",
      "step 365: loss=2.4499664306640625\n",
      "step 366: loss=2.402545928955078\n",
      "step 367: loss=2.4566781520843506\n",
      "step 368: loss=2.4329309463500977\n",
      "step 369: loss=2.4263837337493896\n",
      "step 370: loss=2.3912405967712402\n",
      "step 371: loss=2.4455692768096924\n",
      "step 372: loss=2.4241085052490234\n",
      "step 373: loss=2.551518201828003\n",
      "step 374: loss=2.512552261352539\n",
      "step 375: loss=2.4878759384155273\n",
      "step 376: loss=2.397948980331421\n",
      "step 377: loss=2.3903799057006836\n",
      "step 378: loss=2.407289743423462\n",
      "step 379: loss=2.3729660511016846\n",
      "step 380: loss=2.4717092514038086\n",
      "step 381: loss=2.6281139850616455\n",
      "step 382: loss=2.3908770084381104\n",
      "step 383: loss=2.4578375816345215\n",
      "step 384: loss=2.4924066066741943\n",
      "step 385: loss=2.5656256675720215\n",
      "step 386: loss=2.5396628379821777\n",
      "step 387: loss=2.669271230697632\n",
      "step 388: loss=2.497262477874756\n",
      "step 389: loss=2.4594063758850098\n",
      "step 390: loss=2.4610962867736816\n",
      "step 391: loss=2.5159354209899902\n",
      "step 392: loss=2.4802424907684326\n",
      "step 393: loss=2.467424154281616\n",
      "step 394: loss=2.424487352371216\n",
      "step 395: loss=2.3080289363861084\n",
      "step 396: loss=2.40659761428833\n",
      "step 397: loss=2.5032122135162354\n",
      "step 398: loss=2.3718152046203613\n",
      "step 399: loss=2.5462377071380615\n",
      "step 400: loss=2.5745158195495605\n",
      "step 401: loss=2.399885892868042\n",
      "step 402: loss=2.4647250175476074\n",
      "step 403: loss=2.532038927078247\n",
      "step 404: loss=2.4966890811920166\n",
      "step 405: loss=2.5609655380249023\n",
      "step 406: loss=2.379638195037842\n",
      "step 407: loss=2.4267232418060303\n",
      "step 408: loss=2.439467668533325\n",
      "step 409: loss=2.379018545150757\n",
      "step 410: loss=2.5722861289978027\n",
      "step 411: loss=2.4479079246520996\n",
      "step 412: loss=2.4012181758880615\n",
      "step 413: loss=2.4783482551574707\n",
      "step 414: loss=2.302882432937622\n",
      "step 415: loss=2.411499261856079\n",
      "step 416: loss=2.425609827041626\n",
      "step 417: loss=2.4522132873535156\n",
      "step 418: loss=2.4568734169006348\n",
      "step 419: loss=2.479945421218872\n",
      "step 420: loss=2.4822418689727783\n",
      "step 421: loss=2.5105934143066406\n",
      "step 422: loss=2.4145023822784424\n",
      "step 423: loss=2.4852206707000732\n",
      "step 424: loss=2.3990318775177\n",
      "step 425: loss=2.5252225399017334\n",
      "step 426: loss=2.5224337577819824\n",
      "step 427: loss=2.3164467811584473\n",
      "step 428: loss=2.445484161376953\n",
      "step 429: loss=2.4023730754852295\n",
      "step 430: loss=2.4741053581237793\n",
      "step 431: loss=2.4533956050872803\n",
      "step 432: loss=2.474010467529297\n",
      "step 433: loss=2.5075278282165527\n",
      "step 434: loss=2.5547120571136475\n",
      "step 435: loss=2.4486992359161377\n",
      "step 436: loss=2.5019149780273438\n",
      "step 437: loss=2.462885618209839\n",
      "step 438: loss=2.5207014083862305\n",
      "step 439: loss=2.4549105167388916\n",
      "step 440: loss=2.583989381790161\n",
      "step 441: loss=2.476829767227173\n",
      "step 442: loss=2.4947924613952637\n",
      "step 443: loss=2.435086488723755\n",
      "step 444: loss=2.4995274543762207\n",
      "step 445: loss=2.436785936355591\n",
      "step 446: loss=2.3493738174438477\n",
      "step 447: loss=2.4329020977020264\n",
      "step 448: loss=2.510753870010376\n",
      "step 449: loss=2.503959894180298\n",
      "step 450: loss=2.49194598197937\n",
      "step 451: loss=2.4976212978363037\n",
      "step 452: loss=2.489272117614746\n",
      "step 453: loss=2.4642059803009033\n",
      "step 454: loss=2.4748165607452393\n",
      "step 455: loss=2.512650489807129\n",
      "step 456: loss=2.518733501434326\n",
      "step 457: loss=2.398024320602417\n",
      "step 458: loss=2.4345204830169678\n",
      "step 459: loss=2.488675117492676\n",
      "step 460: loss=2.4042587280273438\n",
      "step 461: loss=2.4750630855560303\n",
      "step 462: loss=2.6756834983825684\n",
      "step 463: loss=2.519571542739868\n",
      "step 464: loss=2.516010284423828\n",
      "step 465: loss=2.54364275932312\n",
      "step 466: loss=2.410580635070801\n",
      "step 467: loss=2.359703779220581\n",
      "step 468: loss=2.4587795734405518\n",
      "step 469: loss=2.4714462757110596\n",
      "step 470: loss=2.396340847015381\n",
      "step 471: loss=2.5067923069000244\n",
      "step 472: loss=2.4797472953796387\n",
      "step 473: loss=2.4730923175811768\n",
      "step 474: loss=2.500046730041504\n",
      "step 475: loss=2.5339372158050537\n",
      "step 476: loss=2.4133801460266113\n",
      "step 477: loss=2.575453758239746\n",
      "step 478: loss=2.4805397987365723\n",
      "step 479: loss=2.4933416843414307\n",
      "step 480: loss=2.4236114025115967\n",
      "step 481: loss=2.436948537826538\n",
      "step 482: loss=2.505286931991577\n",
      "step 483: loss=2.4238953590393066\n",
      "step 484: loss=2.453012466430664\n",
      "step 485: loss=2.4325177669525146\n",
      "step 486: loss=2.434840679168701\n",
      "step 487: loss=2.521383047103882\n",
      "step 488: loss=2.413461685180664\n",
      "step 489: loss=2.4013917446136475\n",
      "step 490: loss=2.57568359375\n",
      "step 491: loss=2.474764823913574\n",
      "step 492: loss=2.487487316131592\n",
      "step 493: loss=2.5536351203918457\n",
      "step 494: loss=2.56201171875\n",
      "step 495: loss=2.436187267303467\n",
      "step 496: loss=2.579876184463501\n",
      "step 497: loss=2.482959747314453\n",
      "step 498: loss=2.503183126449585\n",
      "step 499: loss=2.488495349884033\n",
      "step 500: loss=2.4143502712249756\n",
      "step 501: loss=2.4612479209899902\n",
      "step 502: loss=2.462507486343384\n",
      "step 503: loss=2.5941741466522217\n",
      "step 504: loss=2.4588024616241455\n",
      "step 505: loss=2.6131696701049805\n",
      "step 506: loss=2.465508222579956\n",
      "step 507: loss=2.5417184829711914\n",
      "step 508: loss=2.415006160736084\n",
      "step 509: loss=2.5208795070648193\n",
      "step 510: loss=2.6128621101379395\n",
      "step 511: loss=2.46639084815979\n",
      "step 512: loss=2.4284379482269287\n",
      "step 513: loss=2.5287418365478516\n",
      "step 514: loss=2.4768431186676025\n",
      "step 515: loss=2.5689914226531982\n",
      "step 516: loss=2.5009653568267822\n",
      "step 517: loss=2.479578971862793\n",
      "step 518: loss=2.4659314155578613\n",
      "step 519: loss=2.5270884037017822\n",
      "step 520: loss=2.5803751945495605\n",
      "step 521: loss=2.486781597137451\n",
      "step 522: loss=2.509165048599243\n",
      "step 523: loss=2.457266330718994\n",
      "step 524: loss=2.4272983074188232\n",
      "step 525: loss=2.4888997077941895\n",
      "step 526: loss=2.55899715423584\n",
      "step 527: loss=2.5674386024475098\n",
      "step 528: loss=2.461390733718872\n",
      "step 529: loss=2.3919596672058105\n",
      "step 530: loss=2.388383626937866\n",
      "step 531: loss=2.355180263519287\n",
      "step 532: loss=2.392047166824341\n",
      "step 533: loss=2.482254981994629\n",
      "step 534: loss=2.5415749549865723\n",
      "step 535: loss=2.5068507194519043\n",
      "step 536: loss=2.5482418537139893\n",
      "step 537: loss=2.4125449657440186\n",
      "step 538: loss=2.4914145469665527\n",
      "step 539: loss=2.485811471939087\n",
      "step 540: loss=2.61803936958313\n",
      "step 541: loss=2.4196815490722656\n",
      "step 542: loss=2.5181267261505127\n",
      "step 543: loss=2.44049072265625\n",
      "step 544: loss=2.555510997772217\n",
      "step 545: loss=2.454808235168457\n",
      "step 546: loss=2.460054874420166\n",
      "step 547: loss=2.44478702545166\n",
      "step 548: loss=2.5784523487091064\n",
      "step 549: loss=2.4487760066986084\n",
      "step 550: loss=2.4365322589874268\n",
      "step 551: loss=2.522066831588745\n",
      "step 552: loss=2.494002103805542\n",
      "step 553: loss=2.38811993598938\n",
      "step 554: loss=2.450815200805664\n",
      "step 555: loss=2.394298791885376\n",
      "step 556: loss=2.421967029571533\n",
      "step 557: loss=2.5273313522338867\n",
      "step 558: loss=2.5642828941345215\n",
      "step 559: loss=2.3584110736846924\n",
      "step 560: loss=2.53715443611145\n",
      "step 561: loss=2.4842045307159424\n",
      "step 562: loss=2.4106671810150146\n",
      "step 563: loss=2.5015923976898193\n",
      "step 564: loss=2.4840381145477295\n",
      "step 565: loss=2.429910659790039\n",
      "step 566: loss=2.470341205596924\n",
      "step 567: loss=2.6063661575317383\n",
      "step 568: loss=2.4313766956329346\n",
      "step 569: loss=2.539597988128662\n",
      "step 570: loss=2.388134479522705\n",
      "step 571: loss=2.4402427673339844\n",
      "step 572: loss=2.421006441116333\n",
      "step 573: loss=2.443411350250244\n",
      "step 574: loss=2.471255302429199\n",
      "step 575: loss=2.513577461242676\n",
      "step 576: loss=2.4593071937561035\n",
      "step 577: loss=2.4300177097320557\n",
      "step 578: loss=2.3945229053497314\n",
      "step 579: loss=2.542294502258301\n",
      "step 580: loss=2.4739601612091064\n",
      "step 581: loss=2.560438394546509\n",
      "step 582: loss=2.4929447174072266\n",
      "step 583: loss=2.5055696964263916\n",
      "step 584: loss=2.429938554763794\n",
      "step 585: loss=2.375824451446533\n",
      "step 586: loss=2.4609315395355225\n",
      "step 587: loss=2.5064027309417725\n",
      "step 588: loss=2.597003936767578\n",
      "step 589: loss=2.5117993354797363\n",
      "step 590: loss=2.526451587677002\n",
      "step 591: loss=2.440774440765381\n",
      "step 592: loss=2.5253801345825195\n",
      "step 593: loss=2.459230422973633\n",
      "step 594: loss=2.3519437313079834\n",
      "step 595: loss=2.499939441680908\n",
      "step 596: loss=2.4966788291931152\n",
      "step 597: loss=2.5006422996520996\n",
      "step 598: loss=2.4126551151275635\n",
      "step 599: loss=2.4092628955841064\n",
      "step 600: loss=2.418973445892334\n",
      "step 601: loss=2.5607333183288574\n",
      "step 602: loss=2.492365598678589\n",
      "step 603: loss=2.4370598793029785\n",
      "step 604: loss=2.4748411178588867\n",
      "step 605: loss=2.5170161724090576\n",
      "step 606: loss=2.48781418800354\n",
      "step 607: loss=2.421841859817505\n",
      "step 608: loss=2.609240770339966\n",
      "step 609: loss=2.429319381713867\n",
      "step 610: loss=2.443197250366211\n",
      "step 611: loss=2.55924654006958\n",
      "step 612: loss=2.4047012329101562\n",
      "step 613: loss=2.521148204803467\n",
      "step 614: loss=2.4221489429473877\n",
      "step 615: loss=2.5146262645721436\n",
      "step 616: loss=2.563718557357788\n",
      "step 617: loss=2.5914828777313232\n",
      "step 618: loss=2.5018346309661865\n",
      "step 619: loss=2.496772050857544\n",
      "step 620: loss=2.493626356124878\n",
      "step 621: loss=2.3170435428619385\n",
      "step 622: loss=2.504290819168091\n",
      "step 623: loss=2.5413990020751953\n",
      "step 624: loss=2.528041362762451\n",
      "step 625: loss=2.4006617069244385\n",
      "step 626: loss=2.543757438659668\n",
      "step 627: loss=2.465458393096924\n",
      "step 628: loss=2.5257933139801025\n",
      "step 629: loss=2.5050017833709717\n",
      "step 630: loss=2.485452651977539\n",
      "step 631: loss=2.4209654331207275\n",
      "step 632: loss=2.4700093269348145\n",
      "step 633: loss=2.4231314659118652\n",
      "step 634: loss=2.4311251640319824\n",
      "step 635: loss=2.4259209632873535\n",
      "step 636: loss=2.3621551990509033\n",
      "step 637: loss=2.590826988220215\n",
      "step 638: loss=2.4419116973876953\n",
      "step 639: loss=2.3635332584381104\n",
      "step 640: loss=2.4922375679016113\n",
      "step 641: loss=2.443519353866577\n",
      "step 642: loss=2.490410804748535\n",
      "step 643: loss=2.4327571392059326\n",
      "step 644: loss=2.4915711879730225\n",
      "step 645: loss=2.550546407699585\n",
      "step 646: loss=2.4721925258636475\n",
      "step 647: loss=2.4913628101348877\n",
      "step 648: loss=2.4598655700683594\n",
      "step 649: loss=2.412783145904541\n",
      "step 650: loss=2.512331962585449\n",
      "step 651: loss=2.474550247192383\n",
      "step 652: loss=2.4984991550445557\n",
      "step 653: loss=2.4810667037963867\n",
      "step 654: loss=2.4081082344055176\n",
      "step 655: loss=2.4468696117401123\n",
      "step 656: loss=2.410844326019287\n",
      "step 657: loss=2.5783534049987793\n",
      "step 658: loss=2.5093295574188232\n",
      "step 659: loss=2.5221822261810303\n",
      "step 660: loss=2.530325412750244\n",
      "step 661: loss=2.5817453861236572\n",
      "step 662: loss=2.561152935028076\n",
      "step 663: loss=2.4589180946350098\n",
      "step 664: loss=2.4297428131103516\n",
      "step 665: loss=2.6085503101348877\n",
      "step 666: loss=2.529353618621826\n",
      "step 667: loss=2.4332261085510254\n",
      "step 668: loss=2.452805519104004\n",
      "step 669: loss=2.6139824390411377\n",
      "step 670: loss=2.4669463634490967\n",
      "step 671: loss=2.4888761043548584\n",
      "step 672: loss=2.542206048965454\n",
      "step 673: loss=2.5563771724700928\n",
      "step 674: loss=2.423124074935913\n",
      "step 675: loss=2.5352423191070557\n",
      "step 676: loss=2.4821321964263916\n",
      "step 677: loss=2.4698612689971924\n",
      "step 678: loss=2.408418893814087\n",
      "step 679: loss=2.5637052059173584\n",
      "step 680: loss=2.4901437759399414\n",
      "step 681: loss=2.4751691818237305\n",
      "step 682: loss=2.524358034133911\n",
      "step 683: loss=2.4764161109924316\n",
      "step 684: loss=2.538834571838379\n",
      "step 685: loss=2.4383137226104736\n",
      "step 686: loss=2.5217554569244385\n",
      "step 687: loss=2.5670058727264404\n",
      "step 688: loss=2.5197627544403076\n",
      "step 689: loss=2.397561550140381\n",
      "step 690: loss=2.4195940494537354\n",
      "step 691: loss=2.4412002563476562\n",
      "step 692: loss=2.4073877334594727\n",
      "step 693: loss=2.4265284538269043\n",
      "step 694: loss=2.5232350826263428\n",
      "step 695: loss=2.5107834339141846\n",
      "step 696: loss=2.422112464904785\n",
      "step 697: loss=2.4390854835510254\n",
      "step 698: loss=2.461962938308716\n",
      "step 699: loss=2.404740571975708\n",
      "step 700: loss=2.494617462158203\n",
      "step 701: loss=2.4737160205841064\n",
      "step 702: loss=2.36653208732605\n",
      "step 703: loss=2.496049404144287\n",
      "step 704: loss=2.4014742374420166\n",
      "step 705: loss=2.400343894958496\n",
      "step 706: loss=2.4029126167297363\n",
      "step 707: loss=2.571044921875\n",
      "step 708: loss=2.5735561847686768\n",
      "step 709: loss=2.556488275527954\n",
      "step 710: loss=2.4591028690338135\n",
      "step 711: loss=2.486283779144287\n",
      "step 712: loss=2.511460542678833\n",
      "step 713: loss=2.3628621101379395\n",
      "step 714: loss=2.5731542110443115\n",
      "step 715: loss=2.518097400665283\n",
      "step 716: loss=2.396867036819458\n",
      "step 717: loss=2.451587677001953\n",
      "step 718: loss=2.431039571762085\n",
      "step 719: loss=2.283282995223999\n",
      "step 720: loss=2.4730896949768066\n",
      "step 721: loss=2.4638521671295166\n",
      "step 722: loss=2.461097240447998\n",
      "step 723: loss=2.539921760559082\n",
      "step 724: loss=2.581125020980835\n",
      "step 725: loss=2.515484094619751\n",
      "step 726: loss=2.369546413421631\n",
      "step 727: loss=2.5257089138031006\n",
      "step 728: loss=2.5836663246154785\n",
      "step 729: loss=2.5093326568603516\n",
      "step 730: loss=2.6087470054626465\n",
      "step 731: loss=2.5937116146087646\n",
      "step 732: loss=2.34964919090271\n",
      "step 733: loss=2.5611138343811035\n",
      "step 734: loss=2.482560157775879\n",
      "step 735: loss=2.3675148487091064\n",
      "step 736: loss=2.5069918632507324\n",
      "step 737: loss=2.359363317489624\n",
      "step 738: loss=2.5274481773376465\n",
      "step 739: loss=2.5081663131713867\n",
      "step 740: loss=2.588137626647949\n",
      "step 741: loss=2.543250799179077\n",
      "step 742: loss=2.4769256114959717\n",
      "step 743: loss=2.563261032104492\n",
      "step 744: loss=2.356827974319458\n",
      "step 745: loss=2.378756284713745\n",
      "step 746: loss=2.4204726219177246\n",
      "step 747: loss=2.4030508995056152\n",
      "step 748: loss=2.44905424118042\n",
      "step 749: loss=2.382445812225342\n",
      "step 750: loss=2.321463108062744\n",
      "step 751: loss=2.5636532306671143\n",
      "step 752: loss=2.5833542346954346\n",
      "step 753: loss=2.592986583709717\n",
      "step 754: loss=2.4232523441314697\n",
      "step 755: loss=2.529085159301758\n",
      "step 756: loss=2.5334866046905518\n",
      "step 757: loss=2.4216959476470947\n",
      "step 758: loss=2.5205562114715576\n",
      "step 759: loss=2.555913209915161\n",
      "step 760: loss=2.39412784576416\n",
      "step 761: loss=2.538374662399292\n",
      "step 762: loss=2.4916300773620605\n",
      "step 763: loss=2.5290286540985107\n",
      "step 764: loss=2.4948365688323975\n",
      "step 765: loss=2.575960874557495\n",
      "step 766: loss=2.4002671241760254\n",
      "step 767: loss=2.46081805229187\n",
      "step 768: loss=2.5730559825897217\n",
      "step 769: loss=2.3797061443328857\n",
      "step 770: loss=2.5088212490081787\n",
      "step 771: loss=2.442864418029785\n",
      "step 772: loss=2.4538168907165527\n",
      "step 773: loss=2.45654296875\n",
      "step 774: loss=2.402510166168213\n",
      "step 775: loss=2.522968053817749\n",
      "step 776: loss=2.4678852558135986\n",
      "step 777: loss=2.449773073196411\n",
      "step 778: loss=2.438955068588257\n",
      "step 779: loss=2.424147605895996\n",
      "step 780: loss=2.487438917160034\n",
      "step 781: loss=2.5280797481536865\n",
      "step 782: loss=2.5124237537384033\n",
      "step 783: loss=2.4130728244781494\n",
      "step 784: loss=2.551583766937256\n",
      "step 785: loss=2.3381781578063965\n",
      "step 786: loss=2.4478979110717773\n",
      "step 787: loss=2.42960262298584\n",
      "step 788: loss=2.409168243408203\n",
      "step 789: loss=2.4121909141540527\n",
      "step 790: loss=2.3875434398651123\n",
      "step 791: loss=2.4861159324645996\n",
      "step 792: loss=2.364194631576538\n",
      "step 793: loss=2.3813157081604004\n",
      "step 794: loss=2.398996353149414\n",
      "step 795: loss=2.569213628768921\n",
      "step 796: loss=2.499877452850342\n",
      "step 797: loss=2.456491231918335\n",
      "step 798: loss=2.4699254035949707\n",
      "step 799: loss=2.2895452976226807\n",
      "step 800: loss=2.3975775241851807\n",
      "step 801: loss=2.483323812484741\n",
      "step 802: loss=2.44674015045166\n",
      "step 803: loss=2.5971858501434326\n",
      "step 804: loss=2.4627020359039307\n",
      "step 805: loss=2.4528703689575195\n",
      "step 806: loss=2.5323140621185303\n",
      "step 807: loss=2.3829519748687744\n",
      "step 808: loss=2.606724739074707\n",
      "step 809: loss=2.398130416870117\n",
      "step 810: loss=2.4609320163726807\n",
      "step 811: loss=2.515044927597046\n",
      "step 812: loss=2.4141552448272705\n",
      "step 813: loss=2.442014694213867\n",
      "step 814: loss=2.6014437675476074\n",
      "step 815: loss=2.3686373233795166\n",
      "step 816: loss=2.426743268966675\n",
      "step 817: loss=2.510484218597412\n",
      "step 818: loss=2.4876644611358643\n",
      "step 819: loss=2.484863519668579\n",
      "step 820: loss=2.3862295150756836\n",
      "step 821: loss=2.448190689086914\n",
      "step 822: loss=2.4278125762939453\n",
      "step 823: loss=2.4906463623046875\n",
      "step 824: loss=2.485372304916382\n",
      "step 825: loss=2.3962767124176025\n",
      "step 826: loss=2.442270278930664\n",
      "step 827: loss=2.563405752182007\n",
      "step 828: loss=2.5989582538604736\n",
      "step 829: loss=2.4455344676971436\n",
      "step 830: loss=2.5017828941345215\n",
      "step 831: loss=2.4877216815948486\n",
      "step 832: loss=2.4258289337158203\n",
      "step 833: loss=2.5258872509002686\n",
      "step 834: loss=2.488497495651245\n",
      "step 835: loss=2.489920139312744\n",
      "step 836: loss=2.4710628986358643\n",
      "step 837: loss=2.4765961170196533\n",
      "step 838: loss=2.5875515937805176\n",
      "step 839: loss=2.574127674102783\n",
      "step 840: loss=2.5511815547943115\n",
      "step 841: loss=2.4514598846435547\n",
      "step 842: loss=2.352537155151367\n",
      "step 843: loss=2.4176900386810303\n",
      "step 844: loss=2.5299689769744873\n",
      "step 845: loss=2.4928696155548096\n",
      "step 846: loss=2.510970115661621\n",
      "step 847: loss=2.4163198471069336\n",
      "step 848: loss=2.4353160858154297\n",
      "step 849: loss=2.5755767822265625\n",
      "step 850: loss=2.485201835632324\n",
      "step 851: loss=2.4468297958374023\n",
      "step 852: loss=2.322232246398926\n",
      "step 853: loss=2.5042576789855957\n",
      "step 854: loss=2.5039751529693604\n",
      "step 855: loss=2.3377740383148193\n",
      "step 856: loss=2.640186309814453\n",
      "step 857: loss=2.5499143600463867\n",
      "step 858: loss=2.497596502304077\n",
      "step 859: loss=2.3536860942840576\n",
      "step 860: loss=2.498687744140625\n",
      "step 861: loss=2.3782131671905518\n",
      "step 862: loss=2.5669782161712646\n",
      "step 863: loss=2.46101450920105\n",
      "step 864: loss=2.433506965637207\n",
      "step 865: loss=2.352660655975342\n",
      "step 866: loss=2.4349124431610107\n",
      "step 867: loss=2.4870553016662598\n",
      "step 868: loss=2.542691230773926\n",
      "step 869: loss=2.4053354263305664\n",
      "step 870: loss=2.45278263092041\n",
      "step 871: loss=2.4263834953308105\n",
      "step 872: loss=2.4762957096099854\n",
      "step 873: loss=2.5182321071624756\n",
      "step 874: loss=2.41556978225708\n",
      "step 875: loss=2.532475233078003\n",
      "step 876: loss=2.427083730697632\n",
      "step 877: loss=2.3638055324554443\n",
      "step 878: loss=2.468785524368286\n",
      "step 879: loss=2.555522918701172\n",
      "step 880: loss=2.631460666656494\n",
      "step 881: loss=2.415022134780884\n",
      "step 882: loss=2.647230386734009\n",
      "step 883: loss=2.499904155731201\n",
      "step 884: loss=2.5232889652252197\n",
      "step 885: loss=2.443549871444702\n",
      "step 886: loss=2.434736490249634\n",
      "step 887: loss=2.3822994232177734\n",
      "step 888: loss=2.538386583328247\n",
      "step 889: loss=2.6024112701416016\n",
      "step 890: loss=2.5798251628875732\n",
      "step 891: loss=2.522724151611328\n",
      "step 892: loss=2.4747207164764404\n",
      "step 893: loss=2.467341423034668\n",
      "step 894: loss=2.355286121368408\n",
      "step 895: loss=2.620466947555542\n",
      "step 896: loss=2.4081978797912598\n",
      "step 897: loss=2.364476203918457\n",
      "step 898: loss=2.3923816680908203\n",
      "step 899: loss=2.531818151473999\n",
      "step 900: loss=2.5384695529937744\n",
      "step 901: loss=2.4317193031311035\n",
      "step 902: loss=2.4492990970611572\n",
      "step 903: loss=2.6017332077026367\n",
      "step 904: loss=2.465294361114502\n",
      "step 905: loss=2.3274710178375244\n",
      "step 906: loss=2.2659096717834473\n",
      "step 907: loss=2.4122564792633057\n",
      "step 908: loss=2.4780683517456055\n",
      "step 909: loss=2.403698205947876\n",
      "step 910: loss=2.427889585494995\n",
      "step 911: loss=2.425917625427246\n",
      "step 912: loss=2.4358887672424316\n",
      "step 913: loss=2.4421894550323486\n",
      "step 914: loss=2.5719237327575684\n",
      "step 915: loss=2.5224385261535645\n",
      "step 916: loss=2.466265916824341\n",
      "step 917: loss=2.590773820877075\n",
      "step 918: loss=2.397172451019287\n",
      "step 919: loss=2.420236110687256\n",
      "step 920: loss=2.5630908012390137\n",
      "step 921: loss=2.539987325668335\n",
      "step 922: loss=2.292915105819702\n",
      "step 923: loss=2.4970743656158447\n",
      "step 924: loss=2.459927558898926\n",
      "step 925: loss=2.3993496894836426\n",
      "step 926: loss=2.477168560028076\n",
      "step 927: loss=2.4279072284698486\n",
      "step 928: loss=2.4111745357513428\n",
      "step 929: loss=2.5763182640075684\n",
      "step 930: loss=2.4750730991363525\n",
      "step 931: loss=2.4654059410095215\n",
      "step 932: loss=2.5612337589263916\n",
      "step 933: loss=2.424238920211792\n",
      "step 934: loss=2.4010658264160156\n",
      "step 935: loss=2.519576072692871\n",
      "step 936: loss=2.540395736694336\n",
      "step 937: loss=2.383122444152832\n",
      "step 938: loss=2.3824596405029297\n",
      "step 939: loss=2.4304888248443604\n",
      "step 940: loss=2.5050384998321533\n",
      "step 941: loss=2.5135316848754883\n",
      "step 942: loss=2.5692825317382812\n",
      "step 943: loss=2.367753267288208\n",
      "step 944: loss=2.438685894012451\n",
      "step 945: loss=2.4390523433685303\n",
      "step 946: loss=2.6184756755828857\n",
      "step 947: loss=2.4686927795410156\n",
      "step 948: loss=2.3825454711914062\n",
      "step 949: loss=2.5142874717712402\n",
      "step 950: loss=2.4495675563812256\n",
      "step 951: loss=2.4947094917297363\n",
      "step 952: loss=2.493347406387329\n",
      "step 953: loss=2.3599729537963867\n",
      "step 954: loss=2.539435863494873\n",
      "step 955: loss=2.373210906982422\n",
      "step 956: loss=2.5646257400512695\n",
      "step 957: loss=2.563040018081665\n",
      "step 958: loss=2.454435110092163\n",
      "step 959: loss=2.434678316116333\n",
      "step 960: loss=2.4253499507904053\n",
      "step 961: loss=2.5049216747283936\n",
      "step 962: loss=2.555194139480591\n",
      "step 963: loss=2.5163755416870117\n",
      "step 964: loss=2.5479636192321777\n",
      "step 965: loss=2.536900520324707\n",
      "step 966: loss=2.4698657989501953\n",
      "step 967: loss=2.4987189769744873\n",
      "step 968: loss=2.6242001056671143\n",
      "step 969: loss=2.5164194107055664\n",
      "step 970: loss=2.4671077728271484\n",
      "step 971: loss=2.4272472858428955\n",
      "step 972: loss=2.448728084564209\n",
      "step 973: loss=2.3398396968841553\n",
      "step 974: loss=2.5375261306762695\n",
      "step 975: loss=2.5701725482940674\n",
      "step 976: loss=2.5579705238342285\n",
      "step 977: loss=2.417790412902832\n",
      "step 978: loss=2.5135254859924316\n",
      "step 979: loss=2.4467597007751465\n",
      "step 980: loss=2.5186469554901123\n",
      "step 981: loss=2.3701722621917725\n",
      "step 982: loss=2.552830219268799\n",
      "step 983: loss=2.5148913860321045\n",
      "step 984: loss=2.383654832839966\n",
      "step 985: loss=2.455533981323242\n",
      "step 986: loss=2.5329933166503906\n",
      "step 987: loss=2.3961358070373535\n",
      "step 988: loss=2.524442672729492\n",
      "step 989: loss=2.488722801208496\n",
      "step 990: loss=2.5458643436431885\n",
      "step 991: loss=2.4630160331726074\n",
      "step 992: loss=2.6300740242004395\n",
      "step 993: loss=2.49564528465271\n",
      "step 994: loss=2.386892795562744\n",
      "step 995: loss=2.526182174682617\n",
      "step 996: loss=2.4000706672668457\n",
      "step 997: loss=2.546250581741333\n",
      "step 998: loss=2.441757917404175\n",
      "step 999: loss=2.3487656116485596\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(1000):\n",
    "    \n",
    "    xb, yb = get_batch('train')\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    logits, loss = m(xb, yb)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"step {steps}: loss={loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d90e706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ty'GBusi,loard,\n",
      "Ind m, mar.\n",
      "Fo sspok.\n",
      "\n",
      "S:\n",
      "Yondit me co,\n",
      "Au m a br ureeiso l; yost he oroun baveshy h\n",
      "\n",
      "T:\n",
      "\n",
      "M:\n",
      "\n",
      "F:\n",
      "IO:\n",
      "\n",
      "hbe tstr o 're at asmear aru mangis heas wngse cknt BENAnd ther maFit\n",
      "TERDUCldut hore to rorce il, d r whemin t t awleig, stisonk.\n",
      "n int athay an, s f AUSese,\n",
      "\n",
      "\n",
      "Fin,\n",
      "\n",
      "\n",
      "OKELYof d ETh;\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e2c0a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "724c07fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1]\n",
    "        xbow[b,t] = torch.mean(xprev, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8685973e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(dim=1, keepdim=True)\n",
    "wei\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c31625c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cbdff502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "28f821b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "=========\n",
      "tensor([[5., 7.],\n",
      "        [2., 0.],\n",
      "        [5., 3.]])\n",
      "=========\n",
      "tensor([[5.0000, 7.0000],\n",
      "        [3.5000, 3.5000],\n",
      "        [4.0000, 3.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / torch.sum(a, dim=1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a@b\n",
    "print('=========')\n",
    "print(a)\n",
    "print('=========')\n",
    "print(b)\n",
    "print('=========')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "db7eb657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "876ebc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow2, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "881e1828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
    "\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "# out = wei @ x\n",
    "\n",
    "v= value(x)\n",
    "out = wei @ v # (B, T, T) @ (B, T, 16) -> (B, T, 16)\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "102eea59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb75b24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5  # 这里乘以 head_size 的负 1/2 次方，是为了进行缩放（\"Scaled Dot-Product Attention\"），防止随着维度变大，点积值过大导致 softmax 梯度过小（变平），有助于保持数值稳定性"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
