{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc9773ac",
   "metadata": {},
   "source": [
    "# 训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ffa85c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-07 20:38:47--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.1’\n",
      "\n",
      "input.txt.1         100%[===================>]   1.06M  1.03MB/s    in 1.0s    \n",
      "\n",
      "2025-12-07 20:38:49 (1.03 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b5faf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77311a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters:\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb765aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2935502f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c16bc9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "# 这里，我们为字符集中的每个字符分配一个唯一的数字ID\n",
    "stoi = { ch:i for i,ch in enumerate(chars)}    # 字符到索引（string to index）的映射\n",
    "itos = { i:ch for i,ch in enumerate(chars)}    # 索引到字符（index to string）的映射\n",
    "\n",
    "# 定义一个编码函数，将字符串转换为数字ID序列，方便神经网络处理\n",
    "encodec = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encodec(\"hello\"))\n",
    "print(decode(encodec(\"hello\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8bc131d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encodec(text), dtype=torch.long)\n",
    "\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41c5863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6e2d43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45818b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is 47\n",
      "when input is tensor([18, 47]) the target is 56\n",
      "when input is tensor([18, 47, 56]) the target is 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a423c278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) \n",
      " torch.Size([4, 8])\n",
      "when input is tensor([24]) the target is 43\n",
      "when input is tensor([24, 43]) the target is 58\n",
      "when input is tensor([24, 43, 58]) the target is 5\n",
      "when input is tensor([24, 43, 58,  5]) the target is 57\n",
      "when input is tensor([24, 43, 58,  5, 57]) the target is 1\n",
      "when input is tensor([24, 43, 58,  5, 57,  1]) the target is 46\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46]) the target is 43\n",
      "when input is tensor([24, 43, 58,  5, 57,  1, 46, 43]) the target is 39\n",
      "when input is tensor([44]) the target is 53\n",
      "when input is tensor([44, 53]) the target is 56\n",
      "when input is tensor([44, 53, 56]) the target is 1\n",
      "when input is tensor([44, 53, 56,  1]) the target is 58\n",
      "when input is tensor([44, 53, 56,  1, 58]) the target is 46\n",
      "when input is tensor([44, 53, 56,  1, 58, 46]) the target is 39\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39]) the target is 58\n",
      "when input is tensor([44, 53, 56,  1, 58, 46, 39, 58]) the target is 1\n",
      "when input is tensor([52]) the target is 58\n",
      "when input is tensor([52, 58]) the target is 1\n",
      "when input is tensor([52, 58,  1]) the target is 58\n",
      "when input is tensor([52, 58,  1, 58]) the target is 46\n",
      "when input is tensor([52, 58,  1, 58, 46]) the target is 39\n",
      "when input is tensor([52, 58,  1, 58, 46, 39]) the target is 58\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58]) the target is 1\n",
      "when input is tensor([52, 58,  1, 58, 46, 39, 58,  1]) the target is 46\n",
      "when input is tensor([25]) the target is 17\n",
      "when input is tensor([25, 17]) the target is 27\n",
      "when input is tensor([25, 17, 27]) the target is 10\n",
      "when input is tensor([25, 17, 27, 10]) the target is 0\n",
      "when input is tensor([25, 17, 27, 10,  0]) the target is 21\n",
      "when input is tensor([25, 17, 27, 10,  0, 21]) the target is 1\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1]) the target is 54\n",
      "when input is tensor([25, 17, 27, 10,  0, 21,  1, 54]) the target is 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else test_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(xb.shape, '\\n', yb.shape)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context} the target is {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcae5128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cdc5514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "torch.Size([32, 65])\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# 定义一个“大ram语言模型”类，继承自nn.Module，表示一个能基于前一个词预测下一个词的神经网络模型\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.token_embeding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embeding_table(idx)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html\n",
    "            \n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "        \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # self(idx) 的作用是调用模型的 forward 方法，等价于 self.forward(idx)\n",
    "            # 它返回 (logits, loss)，这里只关心 logits 部分\n",
    "            logits, _ = self(idx)\n",
    "            # logits的返回是在BigramLanguageModel.forward方法里，\n",
    "            # 那里(logits, loss)返回的logits已经是view过后的形状 (B*T, C)。\n",
    "            # 所以这里logits并不是 (B, T, C)，而是 (B*T, C)\n",
    "            # 如果真的要取每个batch的最后一个时间步的logit，需要先把logits reshape 回 (B, T, C)\n",
    "            B, T = idx.shape\n",
    "            logits = logits.view(B, T, -1)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # softmax 返回的是所有类别的概率分布，而不是最大概率的索引\n",
    "            # 如果想要最大索引（即贪婪采样），应该用 argmax\n",
    "            # 用 torch.multinomial 是为了从概率分布中按概率随机采样，增加生成多样性\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(loss)\n",
    "print(logits.shape)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45cb3d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "516d5c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss=4.692410945892334\n",
      "step 1: loss=4.664144515991211\n",
      "step 2: loss=4.765714645385742\n",
      "step 3: loss=4.70655632019043\n",
      "step 4: loss=4.5956573486328125\n",
      "step 5: loss=4.7101240158081055\n",
      "step 6: loss=4.713661193847656\n",
      "step 7: loss=4.686909198760986\n",
      "step 8: loss=4.700076103210449\n",
      "step 9: loss=4.718283653259277\n",
      "step 10: loss=4.715603351593018\n",
      "step 11: loss=4.684308052062988\n",
      "step 12: loss=4.745601177215576\n",
      "step 13: loss=4.735717296600342\n",
      "step 14: loss=4.666238784790039\n",
      "step 15: loss=4.58615255355835\n",
      "step 16: loss=4.714625835418701\n",
      "step 17: loss=4.671982765197754\n",
      "step 18: loss=4.715047359466553\n",
      "step 19: loss=4.74489164352417\n",
      "step 20: loss=4.630162715911865\n",
      "step 21: loss=4.707578182220459\n",
      "step 22: loss=4.670665740966797\n",
      "step 23: loss=4.582583427429199\n",
      "step 24: loss=4.739546298980713\n",
      "step 25: loss=4.674807071685791\n",
      "step 26: loss=4.805595874786377\n",
      "step 27: loss=4.749917507171631\n",
      "step 28: loss=4.691989421844482\n",
      "step 29: loss=4.604404926300049\n",
      "step 30: loss=4.721841335296631\n",
      "step 31: loss=4.741591930389404\n",
      "step 32: loss=4.609963417053223\n",
      "step 33: loss=4.662769794464111\n",
      "step 34: loss=4.730099678039551\n",
      "step 35: loss=4.738433361053467\n",
      "step 36: loss=4.688235282897949\n",
      "step 37: loss=4.639987945556641\n",
      "step 38: loss=4.736632823944092\n",
      "step 39: loss=4.709773540496826\n",
      "step 40: loss=4.736939430236816\n",
      "step 41: loss=4.69184684753418\n",
      "step 42: loss=4.719646453857422\n",
      "step 43: loss=4.752516746520996\n",
      "step 44: loss=4.570086479187012\n",
      "step 45: loss=4.643786907196045\n",
      "step 46: loss=4.699163913726807\n",
      "step 47: loss=4.806960105895996\n",
      "step 48: loss=4.572142601013184\n",
      "step 49: loss=4.717066287994385\n",
      "step 50: loss=4.509502410888672\n",
      "step 51: loss=4.603540897369385\n",
      "step 52: loss=4.6649675369262695\n",
      "step 53: loss=4.712099075317383\n",
      "step 54: loss=4.736577033996582\n",
      "step 55: loss=4.812878131866455\n",
      "step 56: loss=4.596436977386475\n",
      "step 57: loss=4.702690601348877\n",
      "step 58: loss=4.711158752441406\n",
      "step 59: loss=4.636075019836426\n",
      "step 60: loss=4.706498146057129\n",
      "step 61: loss=4.706602573394775\n",
      "step 62: loss=4.776336193084717\n",
      "step 63: loss=4.538954257965088\n",
      "step 64: loss=4.595245838165283\n",
      "step 65: loss=4.648927688598633\n",
      "step 66: loss=4.668923854827881\n",
      "step 67: loss=4.5940351486206055\n",
      "step 68: loss=4.757757186889648\n",
      "step 69: loss=4.683036804199219\n",
      "step 70: loss=4.627960205078125\n",
      "step 71: loss=4.809708118438721\n",
      "step 72: loss=4.720103740692139\n",
      "step 73: loss=4.69432258605957\n",
      "step 74: loss=4.593808650970459\n",
      "step 75: loss=4.627923488616943\n",
      "step 76: loss=4.628483772277832\n",
      "step 77: loss=4.594723224639893\n",
      "step 78: loss=4.667132377624512\n",
      "step 79: loss=4.563992500305176\n",
      "step 80: loss=4.598272323608398\n",
      "step 81: loss=4.718154430389404\n",
      "step 82: loss=4.686450958251953\n",
      "step 83: loss=4.564647197723389\n",
      "step 84: loss=4.66386604309082\n",
      "step 85: loss=4.6551690101623535\n",
      "step 86: loss=4.503818511962891\n",
      "step 87: loss=4.662378311157227\n",
      "step 88: loss=4.597469329833984\n",
      "step 89: loss=4.553940773010254\n",
      "step 90: loss=4.6458845138549805\n",
      "step 91: loss=4.658196926116943\n",
      "step 92: loss=4.652643203735352\n",
      "step 93: loss=4.572150230407715\n",
      "step 94: loss=4.654421806335449\n",
      "step 95: loss=4.505650997161865\n",
      "step 96: loss=4.6306376457214355\n",
      "step 97: loss=4.7071919441223145\n",
      "step 98: loss=4.6614508628845215\n",
      "step 99: loss=4.65630578994751\n",
      "step 100: loss=4.621085166931152\n",
      "step 101: loss=4.699936389923096\n",
      "step 102: loss=4.604835510253906\n",
      "step 103: loss=4.62617826461792\n",
      "step 104: loss=4.598467826843262\n",
      "step 105: loss=4.562093257904053\n",
      "step 106: loss=4.61775541305542\n",
      "step 107: loss=4.545349597930908\n",
      "step 108: loss=4.5832133293151855\n",
      "step 109: loss=4.523378849029541\n",
      "step 110: loss=4.568169593811035\n",
      "step 111: loss=4.605664253234863\n",
      "step 112: loss=4.625260829925537\n",
      "step 113: loss=4.56585168838501\n",
      "step 114: loss=4.675260066986084\n",
      "step 115: loss=4.607123851776123\n",
      "step 116: loss=4.707728385925293\n",
      "step 117: loss=4.575418949127197\n",
      "step 118: loss=4.564972877502441\n",
      "step 119: loss=4.588582515716553\n",
      "step 120: loss=4.592921733856201\n",
      "step 121: loss=4.5604448318481445\n",
      "step 122: loss=4.7858452796936035\n",
      "step 123: loss=4.569169521331787\n",
      "step 124: loss=4.5731329917907715\n",
      "step 125: loss=4.601418495178223\n",
      "step 126: loss=4.576107978820801\n",
      "step 127: loss=4.60406494140625\n",
      "step 128: loss=4.595185279846191\n",
      "step 129: loss=4.515230655670166\n",
      "step 130: loss=4.476064205169678\n",
      "step 131: loss=4.521181583404541\n",
      "step 132: loss=4.655724048614502\n",
      "step 133: loss=4.520238399505615\n",
      "step 134: loss=4.546555042266846\n",
      "step 135: loss=4.582464694976807\n",
      "step 136: loss=4.651482105255127\n",
      "step 137: loss=4.5329389572143555\n",
      "step 138: loss=4.552127361297607\n",
      "step 139: loss=4.610964775085449\n",
      "step 140: loss=4.720569610595703\n",
      "step 141: loss=4.532519817352295\n",
      "step 142: loss=4.590734004974365\n",
      "step 143: loss=4.659023284912109\n",
      "step 144: loss=4.569576740264893\n",
      "step 145: loss=4.501828193664551\n",
      "step 146: loss=4.534751892089844\n",
      "step 147: loss=4.5555877685546875\n",
      "step 148: loss=4.437855243682861\n",
      "step 149: loss=4.584807872772217\n",
      "step 150: loss=4.493810176849365\n",
      "step 151: loss=4.496149063110352\n",
      "step 152: loss=4.61268424987793\n",
      "step 153: loss=4.541433811187744\n",
      "step 154: loss=4.462303161621094\n",
      "step 155: loss=4.494714736938477\n",
      "step 156: loss=4.5445098876953125\n",
      "step 157: loss=4.591465473175049\n",
      "step 158: loss=4.559953212738037\n",
      "step 159: loss=4.522037029266357\n",
      "step 160: loss=4.604924201965332\n",
      "step 161: loss=4.505509853363037\n",
      "step 162: loss=4.600823402404785\n",
      "step 163: loss=4.534110069274902\n",
      "step 164: loss=4.466233730316162\n",
      "step 165: loss=4.4869561195373535\n",
      "step 166: loss=4.499792098999023\n",
      "step 167: loss=4.558073043823242\n",
      "step 168: loss=4.6118950843811035\n",
      "step 169: loss=4.532322406768799\n",
      "step 170: loss=4.494655132293701\n",
      "step 171: loss=4.501111030578613\n",
      "step 172: loss=4.487987518310547\n",
      "step 173: loss=4.500681400299072\n",
      "step 174: loss=4.447858810424805\n",
      "step 175: loss=4.551181793212891\n",
      "step 176: loss=4.5280585289001465\n",
      "step 177: loss=4.5666728019714355\n",
      "step 178: loss=4.546204566955566\n",
      "step 179: loss=4.588383197784424\n",
      "step 180: loss=4.5528364181518555\n",
      "step 181: loss=4.483713626861572\n",
      "step 182: loss=4.496376037597656\n",
      "step 183: loss=4.405621528625488\n",
      "step 184: loss=4.482258319854736\n",
      "step 185: loss=4.484110355377197\n",
      "step 186: loss=4.4490227699279785\n",
      "step 187: loss=4.338376045227051\n",
      "step 188: loss=4.460762977600098\n",
      "step 189: loss=4.561094284057617\n",
      "step 190: loss=4.521778583526611\n",
      "step 191: loss=4.64401912689209\n",
      "step 192: loss=4.514046669006348\n",
      "step 193: loss=4.4592413902282715\n",
      "step 194: loss=4.4573655128479\n",
      "step 195: loss=4.652929782867432\n",
      "step 196: loss=4.569197177886963\n",
      "step 197: loss=4.549731254577637\n",
      "step 198: loss=4.605109691619873\n",
      "step 199: loss=4.531443119049072\n",
      "step 200: loss=4.549462795257568\n",
      "step 201: loss=4.4532389640808105\n",
      "step 202: loss=4.42667293548584\n",
      "step 203: loss=4.428462505340576\n",
      "step 204: loss=4.5073981285095215\n",
      "step 205: loss=4.599745273590088\n",
      "step 206: loss=4.428247451782227\n",
      "step 207: loss=4.516406059265137\n",
      "step 208: loss=4.531017780303955\n",
      "step 209: loss=4.524899005889893\n",
      "step 210: loss=4.43112325668335\n",
      "step 211: loss=4.413572311401367\n",
      "step 212: loss=4.460278511047363\n",
      "step 213: loss=4.394894123077393\n",
      "step 214: loss=4.440585136413574\n",
      "step 215: loss=4.515986442565918\n",
      "step 216: loss=4.501888751983643\n",
      "step 217: loss=4.540366172790527\n",
      "step 218: loss=4.476080417633057\n",
      "step 219: loss=4.480386257171631\n",
      "step 220: loss=4.420384407043457\n",
      "step 221: loss=4.483820915222168\n",
      "step 222: loss=4.512678146362305\n",
      "step 223: loss=4.560887336730957\n",
      "step 224: loss=4.491804599761963\n",
      "step 225: loss=4.480713844299316\n",
      "step 226: loss=4.423666954040527\n",
      "step 227: loss=4.433342456817627\n",
      "step 228: loss=4.417287826538086\n",
      "step 229: loss=4.414680480957031\n",
      "step 230: loss=4.418458461761475\n",
      "step 231: loss=4.457849025726318\n",
      "step 232: loss=4.501733779907227\n",
      "step 233: loss=4.55715799331665\n",
      "step 234: loss=4.420411109924316\n",
      "step 235: loss=4.500718116760254\n",
      "step 236: loss=4.4469828605651855\n",
      "step 237: loss=4.404678821563721\n",
      "step 238: loss=4.448772430419922\n",
      "step 239: loss=4.48320198059082\n",
      "step 240: loss=4.401884078979492\n",
      "step 241: loss=4.445903301239014\n",
      "step 242: loss=4.52253532409668\n",
      "step 243: loss=4.410618305206299\n",
      "step 244: loss=4.45168924331665\n",
      "step 245: loss=4.405672073364258\n",
      "step 246: loss=4.359124660491943\n",
      "step 247: loss=4.418424606323242\n",
      "step 248: loss=4.447394371032715\n",
      "step 249: loss=4.4999918937683105\n",
      "step 250: loss=4.495026111602783\n",
      "step 251: loss=4.441216945648193\n",
      "step 252: loss=4.3983259201049805\n",
      "step 253: loss=4.399979591369629\n",
      "step 254: loss=4.396550178527832\n",
      "step 255: loss=4.515158176422119\n",
      "step 256: loss=4.436650276184082\n",
      "step 257: loss=4.254152774810791\n",
      "step 258: loss=4.4689860343933105\n",
      "step 259: loss=4.4714837074279785\n",
      "step 260: loss=4.331572532653809\n",
      "step 261: loss=4.43186616897583\n",
      "step 262: loss=4.361902713775635\n",
      "step 263: loss=4.399670124053955\n",
      "step 264: loss=4.441038608551025\n",
      "step 265: loss=4.391478061676025\n",
      "step 266: loss=4.487184524536133\n",
      "step 267: loss=4.294976234436035\n",
      "step 268: loss=4.41543436050415\n",
      "step 269: loss=4.420255661010742\n",
      "step 270: loss=4.351208209991455\n",
      "step 271: loss=4.358051300048828\n",
      "step 272: loss=4.529325008392334\n",
      "step 273: loss=4.34090518951416\n",
      "step 274: loss=4.488083839416504\n",
      "step 275: loss=4.442676067352295\n",
      "step 276: loss=4.422273635864258\n",
      "step 277: loss=4.372884273529053\n",
      "step 278: loss=4.420283317565918\n",
      "step 279: loss=4.343837261199951\n",
      "step 280: loss=4.380279064178467\n",
      "step 281: loss=4.460439205169678\n",
      "step 282: loss=4.4696574211120605\n",
      "step 283: loss=4.4250054359436035\n",
      "step 284: loss=4.369536876678467\n",
      "step 285: loss=4.380033016204834\n",
      "step 286: loss=4.365662097930908\n",
      "step 287: loss=4.339999675750732\n",
      "step 288: loss=4.48419713973999\n",
      "step 289: loss=4.555813789367676\n",
      "step 290: loss=4.4380035400390625\n",
      "step 291: loss=4.477532386779785\n",
      "step 292: loss=4.378237724304199\n",
      "step 293: loss=4.336991310119629\n",
      "step 294: loss=4.448875427246094\n",
      "step 295: loss=4.361961841583252\n",
      "step 296: loss=4.365668296813965\n",
      "step 297: loss=4.469748020172119\n",
      "step 298: loss=4.392701625823975\n",
      "step 299: loss=4.334584712982178\n",
      "step 300: loss=4.345612049102783\n",
      "step 301: loss=4.306689739227295\n",
      "step 302: loss=4.438986301422119\n",
      "step 303: loss=4.454972267150879\n",
      "step 304: loss=4.497693061828613\n",
      "step 305: loss=4.392993927001953\n",
      "step 306: loss=4.369708061218262\n",
      "step 307: loss=4.354208469390869\n",
      "step 308: loss=4.376923084259033\n",
      "step 309: loss=4.388126850128174\n",
      "step 310: loss=4.346914291381836\n",
      "step 311: loss=4.454780101776123\n",
      "step 312: loss=4.369599342346191\n",
      "step 313: loss=4.400058746337891\n",
      "step 314: loss=4.321812629699707\n",
      "step 315: loss=4.430188179016113\n",
      "step 316: loss=4.363119125366211\n",
      "step 317: loss=4.258288860321045\n",
      "step 318: loss=4.324169635772705\n",
      "step 319: loss=4.358462333679199\n",
      "step 320: loss=4.358053207397461\n",
      "step 321: loss=4.346726417541504\n",
      "step 322: loss=4.4252777099609375\n",
      "step 323: loss=4.3356428146362305\n",
      "step 324: loss=4.356608867645264\n",
      "step 325: loss=4.268974304199219\n",
      "step 326: loss=4.34548282623291\n",
      "step 327: loss=4.391635894775391\n",
      "step 328: loss=4.350566387176514\n",
      "step 329: loss=4.366872310638428\n",
      "step 330: loss=4.406307220458984\n",
      "step 331: loss=4.266676425933838\n",
      "step 332: loss=4.258195877075195\n",
      "step 333: loss=4.2933526039123535\n",
      "step 334: loss=4.3209686279296875\n",
      "step 335: loss=4.332687854766846\n",
      "step 336: loss=4.352051258087158\n",
      "step 337: loss=4.382238864898682\n",
      "step 338: loss=4.383143901824951\n",
      "step 339: loss=4.244317531585693\n",
      "step 340: loss=4.3100972175598145\n",
      "step 341: loss=4.301767826080322\n",
      "step 342: loss=4.462956428527832\n",
      "step 343: loss=4.325232028961182\n",
      "step 344: loss=4.341349124908447\n",
      "step 345: loss=4.296145439147949\n",
      "step 346: loss=4.235404014587402\n",
      "step 347: loss=4.250915050506592\n",
      "step 348: loss=4.308598518371582\n",
      "step 349: loss=4.376663684844971\n",
      "step 350: loss=4.237582683563232\n",
      "step 351: loss=4.350257396697998\n",
      "step 352: loss=4.228846549987793\n",
      "step 353: loss=4.305774211883545\n",
      "step 354: loss=4.354750633239746\n",
      "step 355: loss=4.467280864715576\n",
      "step 356: loss=4.318342685699463\n",
      "step 357: loss=4.338566780090332\n",
      "step 358: loss=4.289865970611572\n",
      "step 359: loss=4.2690815925598145\n",
      "step 360: loss=4.180973529815674\n",
      "step 361: loss=4.387163162231445\n",
      "step 362: loss=4.275718688964844\n",
      "step 363: loss=4.193756580352783\n",
      "step 364: loss=4.348846435546875\n",
      "step 365: loss=4.311291694641113\n",
      "step 366: loss=4.2306294441223145\n",
      "step 367: loss=4.389198303222656\n",
      "step 368: loss=4.380277633666992\n",
      "step 369: loss=4.376614570617676\n",
      "step 370: loss=4.3441596031188965\n",
      "step 371: loss=4.27800989151001\n",
      "step 372: loss=4.265659332275391\n",
      "step 373: loss=4.27650260925293\n",
      "step 374: loss=4.285951614379883\n",
      "step 375: loss=4.260744094848633\n",
      "step 376: loss=4.254425525665283\n",
      "step 377: loss=4.387601852416992\n",
      "step 378: loss=4.293213367462158\n",
      "step 379: loss=4.275619029998779\n",
      "step 380: loss=4.264163017272949\n",
      "step 381: loss=4.214735507965088\n",
      "step 382: loss=4.323124885559082\n",
      "step 383: loss=4.272207260131836\n",
      "step 384: loss=4.2730560302734375\n",
      "step 385: loss=4.332861423492432\n",
      "step 386: loss=4.228231906890869\n",
      "step 387: loss=4.272346496582031\n",
      "step 388: loss=4.331362724304199\n",
      "step 389: loss=4.377825736999512\n",
      "step 390: loss=4.194937705993652\n",
      "step 391: loss=4.29219913482666\n",
      "step 392: loss=4.230798244476318\n",
      "step 393: loss=4.251380920410156\n",
      "step 394: loss=4.296419620513916\n",
      "step 395: loss=4.252346992492676\n",
      "step 396: loss=4.2831315994262695\n",
      "step 397: loss=4.3855204582214355\n",
      "step 398: loss=4.1769537925720215\n",
      "step 399: loss=4.227319717407227\n",
      "step 400: loss=4.25573205947876\n",
      "step 401: loss=4.238279819488525\n",
      "step 402: loss=4.283568382263184\n",
      "step 403: loss=4.299123764038086\n",
      "step 404: loss=4.25726318359375\n",
      "step 405: loss=4.330580711364746\n",
      "step 406: loss=4.313518524169922\n",
      "step 407: loss=4.335666179656982\n",
      "step 408: loss=4.248019695281982\n",
      "step 409: loss=4.212539196014404\n",
      "step 410: loss=4.390359878540039\n",
      "step 411: loss=4.26472806930542\n",
      "step 412: loss=4.235801696777344\n",
      "step 413: loss=4.21953821182251\n",
      "step 414: loss=4.3849310874938965\n",
      "step 415: loss=4.191295146942139\n",
      "step 416: loss=4.29976224899292\n",
      "step 417: loss=4.263235092163086\n",
      "step 418: loss=4.209293365478516\n",
      "step 419: loss=4.223510265350342\n",
      "step 420: loss=4.197415351867676\n",
      "step 421: loss=4.352291584014893\n",
      "step 422: loss=4.358164310455322\n",
      "step 423: loss=4.247671604156494\n",
      "step 424: loss=4.329915523529053\n",
      "step 425: loss=4.220633506774902\n",
      "step 426: loss=4.240585803985596\n",
      "step 427: loss=4.314285755157471\n",
      "step 428: loss=4.2603759765625\n",
      "step 429: loss=4.259975433349609\n",
      "step 430: loss=4.2319464683532715\n",
      "step 431: loss=4.184201240539551\n",
      "step 432: loss=4.277885913848877\n",
      "step 433: loss=4.279393196105957\n",
      "step 434: loss=4.180922508239746\n",
      "step 435: loss=4.270190238952637\n",
      "step 436: loss=4.253904819488525\n",
      "step 437: loss=4.179213047027588\n",
      "step 438: loss=4.2253098487854\n",
      "step 439: loss=4.279184341430664\n",
      "step 440: loss=4.301351547241211\n",
      "step 441: loss=4.229222297668457\n",
      "step 442: loss=4.343070030212402\n",
      "step 443: loss=4.276855945587158\n",
      "step 444: loss=4.191099166870117\n",
      "step 445: loss=4.293545722961426\n",
      "step 446: loss=4.201022148132324\n",
      "step 447: loss=4.206254959106445\n",
      "step 448: loss=4.195919990539551\n",
      "step 449: loss=4.2168192863464355\n",
      "step 450: loss=4.221164703369141\n",
      "step 451: loss=4.163954257965088\n",
      "step 452: loss=4.355230331420898\n",
      "step 453: loss=4.183740139007568\n",
      "step 454: loss=4.180234432220459\n",
      "step 455: loss=4.350985050201416\n",
      "step 456: loss=4.179173946380615\n",
      "step 457: loss=4.266575336456299\n",
      "step 458: loss=4.145872592926025\n",
      "step 459: loss=4.3596954345703125\n",
      "step 460: loss=4.116206169128418\n",
      "step 461: loss=4.278004169464111\n",
      "step 462: loss=4.263311386108398\n",
      "step 463: loss=4.26434850692749\n",
      "step 464: loss=4.247138977050781\n",
      "step 465: loss=4.2241597175598145\n",
      "step 466: loss=4.163585662841797\n",
      "step 467: loss=4.185435771942139\n",
      "step 468: loss=4.255035877227783\n",
      "step 469: loss=4.19374418258667\n",
      "step 470: loss=4.256316661834717\n",
      "step 471: loss=4.281538963317871\n",
      "step 472: loss=4.171205043792725\n",
      "step 473: loss=4.182270526885986\n",
      "step 474: loss=4.194500923156738\n",
      "step 475: loss=4.283398628234863\n",
      "step 476: loss=4.299307823181152\n",
      "step 477: loss=4.241708755493164\n",
      "step 478: loss=4.283669948577881\n",
      "step 479: loss=4.143308639526367\n",
      "step 480: loss=4.15635347366333\n",
      "step 481: loss=4.232965469360352\n",
      "step 482: loss=4.241590976715088\n",
      "step 483: loss=4.03627872467041\n",
      "step 484: loss=4.157522201538086\n",
      "step 485: loss=4.231443405151367\n",
      "step 486: loss=4.147864818572998\n",
      "step 487: loss=4.156970024108887\n",
      "step 488: loss=4.223931789398193\n",
      "step 489: loss=4.105999946594238\n",
      "step 490: loss=4.140515327453613\n",
      "step 491: loss=4.220064163208008\n",
      "step 492: loss=4.173741817474365\n",
      "step 493: loss=4.072698593139648\n",
      "step 494: loss=4.158931255340576\n",
      "step 495: loss=4.1450676918029785\n",
      "step 496: loss=4.184323787689209\n",
      "step 497: loss=4.103063583374023\n",
      "step 498: loss=4.092406272888184\n",
      "step 499: loss=4.1035356521606445\n",
      "step 500: loss=4.214480876922607\n",
      "step 501: loss=4.084356307983398\n",
      "step 502: loss=4.133087158203125\n",
      "step 503: loss=4.223405838012695\n",
      "step 504: loss=4.220446586608887\n",
      "step 505: loss=4.17624568939209\n",
      "step 506: loss=4.128087043762207\n",
      "step 507: loss=4.1905412673950195\n",
      "step 508: loss=4.2345123291015625\n",
      "step 509: loss=4.164053440093994\n",
      "step 510: loss=4.095093250274658\n",
      "step 511: loss=4.139039039611816\n",
      "step 512: loss=4.137739181518555\n",
      "step 513: loss=4.16702127456665\n",
      "step 514: loss=4.160588264465332\n",
      "step 515: loss=4.111112594604492\n",
      "step 516: loss=4.019512176513672\n",
      "step 517: loss=4.21013879776001\n",
      "step 518: loss=4.071273326873779\n",
      "step 519: loss=4.1922783851623535\n",
      "step 520: loss=4.240005016326904\n",
      "step 521: loss=4.245697021484375\n",
      "step 522: loss=4.147686958312988\n",
      "step 523: loss=4.2053656578063965\n",
      "step 524: loss=4.214274883270264\n",
      "step 525: loss=4.0432515144348145\n",
      "step 526: loss=4.149839401245117\n",
      "step 527: loss=4.167660713195801\n",
      "step 528: loss=4.0837626457214355\n",
      "step 529: loss=4.21851110458374\n",
      "step 530: loss=4.211829662322998\n",
      "step 531: loss=4.033554553985596\n",
      "step 532: loss=4.161428928375244\n",
      "step 533: loss=4.165452003479004\n",
      "step 534: loss=4.050341606140137\n",
      "step 535: loss=4.1566925048828125\n",
      "step 536: loss=4.160954475402832\n",
      "step 537: loss=4.093878269195557\n",
      "step 538: loss=4.132650375366211\n",
      "step 539: loss=4.1404008865356445\n",
      "step 540: loss=4.130082130432129\n",
      "step 541: loss=4.175171375274658\n",
      "step 542: loss=4.112301826477051\n",
      "step 543: loss=4.073479175567627\n",
      "step 544: loss=4.167318820953369\n",
      "step 545: loss=4.1959919929504395\n",
      "step 546: loss=4.078147888183594\n",
      "step 547: loss=4.069385528564453\n",
      "step 548: loss=4.063492774963379\n",
      "step 549: loss=4.158783435821533\n",
      "step 550: loss=4.046174049377441\n",
      "step 551: loss=4.117081642150879\n",
      "step 552: loss=4.2450761795043945\n",
      "step 553: loss=4.109870433807373\n",
      "step 554: loss=4.118143081665039\n",
      "step 555: loss=4.16467809677124\n",
      "step 556: loss=4.108386039733887\n",
      "step 557: loss=4.165163040161133\n",
      "step 558: loss=4.048225402832031\n",
      "step 559: loss=4.135279655456543\n",
      "step 560: loss=4.158371925354004\n",
      "step 561: loss=4.126312732696533\n",
      "step 562: loss=3.964298725128174\n",
      "step 563: loss=4.15919303894043\n",
      "step 564: loss=4.119665145874023\n",
      "step 565: loss=4.238160133361816\n",
      "step 566: loss=4.114436626434326\n",
      "step 567: loss=4.085212230682373\n",
      "step 568: loss=4.048520088195801\n",
      "step 569: loss=4.12908411026001\n",
      "step 570: loss=4.118659019470215\n",
      "step 571: loss=4.099023818969727\n",
      "step 572: loss=4.183162689208984\n",
      "step 573: loss=4.058305740356445\n",
      "step 574: loss=4.143056392669678\n",
      "step 575: loss=4.114629745483398\n",
      "step 576: loss=4.0972514152526855\n",
      "step 577: loss=4.099020481109619\n",
      "step 578: loss=4.109713554382324\n",
      "step 579: loss=4.0857157707214355\n",
      "step 580: loss=4.125316143035889\n",
      "step 581: loss=4.175577640533447\n",
      "step 582: loss=4.063840389251709\n",
      "step 583: loss=4.108927249908447\n",
      "step 584: loss=4.104191780090332\n",
      "step 585: loss=4.119007110595703\n",
      "step 586: loss=4.09041166305542\n",
      "step 587: loss=4.067192077636719\n",
      "step 588: loss=4.102412223815918\n",
      "step 589: loss=4.113567352294922\n",
      "step 590: loss=4.192253589630127\n",
      "step 591: loss=4.100231170654297\n",
      "step 592: loss=4.120125770568848\n",
      "step 593: loss=4.143221378326416\n",
      "step 594: loss=4.08186674118042\n",
      "step 595: loss=4.087782382965088\n",
      "step 596: loss=4.074817657470703\n",
      "step 597: loss=4.0610480308532715\n",
      "step 598: loss=4.183136463165283\n",
      "step 599: loss=4.0359954833984375\n",
      "step 600: loss=4.124096870422363\n",
      "step 601: loss=4.125418186187744\n",
      "step 602: loss=4.087498188018799\n",
      "step 603: loss=4.018767356872559\n",
      "step 604: loss=4.111219882965088\n",
      "step 605: loss=4.014806270599365\n",
      "step 606: loss=4.001387596130371\n",
      "step 607: loss=3.984426736831665\n",
      "step 608: loss=4.148003578186035\n",
      "step 609: loss=4.025826454162598\n",
      "step 610: loss=4.057998180389404\n",
      "step 611: loss=3.9568910598754883\n",
      "step 612: loss=4.0929460525512695\n",
      "step 613: loss=4.072330474853516\n",
      "step 614: loss=4.03060245513916\n",
      "step 615: loss=4.048306941986084\n",
      "step 616: loss=4.133228778839111\n",
      "step 617: loss=4.038397789001465\n",
      "step 618: loss=4.125432014465332\n",
      "step 619: loss=4.072927474975586\n",
      "step 620: loss=4.045656204223633\n",
      "step 621: loss=4.1224045753479\n",
      "step 622: loss=4.121188163757324\n",
      "step 623: loss=4.098836898803711\n",
      "step 624: loss=4.161966323852539\n",
      "step 625: loss=4.04520845413208\n",
      "step 626: loss=4.056055068969727\n",
      "step 627: loss=3.9817774295806885\n",
      "step 628: loss=4.114030361175537\n",
      "step 629: loss=4.055196762084961\n",
      "step 630: loss=4.181158065795898\n",
      "step 631: loss=4.015369415283203\n",
      "step 632: loss=4.070910453796387\n",
      "step 633: loss=4.0655999183654785\n",
      "step 634: loss=4.0205183029174805\n",
      "step 635: loss=4.009420871734619\n",
      "step 636: loss=4.009321689605713\n",
      "step 637: loss=4.121938705444336\n",
      "step 638: loss=4.040282726287842\n",
      "step 639: loss=4.006514072418213\n",
      "step 640: loss=3.9769175052642822\n",
      "step 641: loss=3.9687490463256836\n",
      "step 642: loss=4.053492546081543\n",
      "step 643: loss=4.024524211883545\n",
      "step 644: loss=4.1228790283203125\n",
      "step 645: loss=3.9823174476623535\n",
      "step 646: loss=3.9732022285461426\n",
      "step 647: loss=4.015324592590332\n",
      "step 648: loss=4.0347113609313965\n",
      "step 649: loss=4.076888561248779\n",
      "step 650: loss=4.0141777992248535\n",
      "step 651: loss=4.0707106590271\n",
      "step 652: loss=4.029590606689453\n",
      "step 653: loss=4.0675764083862305\n",
      "step 654: loss=3.958583116531372\n",
      "step 655: loss=4.049327850341797\n",
      "step 656: loss=3.9634346961975098\n",
      "step 657: loss=4.109421730041504\n",
      "step 658: loss=4.038191795349121\n",
      "step 659: loss=4.125021934509277\n",
      "step 660: loss=4.031307220458984\n",
      "step 661: loss=4.070112705230713\n",
      "step 662: loss=3.9946374893188477\n",
      "step 663: loss=4.0985188484191895\n",
      "step 664: loss=3.9959030151367188\n",
      "step 665: loss=4.035952568054199\n",
      "step 666: loss=4.017228603363037\n",
      "step 667: loss=4.024997234344482\n",
      "step 668: loss=3.9557952880859375\n",
      "step 669: loss=4.03995418548584\n",
      "step 670: loss=4.067239761352539\n",
      "step 671: loss=4.015563011169434\n",
      "step 672: loss=4.054080486297607\n",
      "step 673: loss=4.0563883781433105\n",
      "step 674: loss=3.9339613914489746\n",
      "step 675: loss=3.991907835006714\n",
      "step 676: loss=3.9665045738220215\n",
      "step 677: loss=4.003228664398193\n",
      "step 678: loss=4.0467000007629395\n",
      "step 679: loss=4.060911178588867\n",
      "step 680: loss=3.9072952270507812\n",
      "step 681: loss=4.017505645751953\n",
      "step 682: loss=4.071604251861572\n",
      "step 683: loss=4.083905220031738\n",
      "step 684: loss=4.046928405761719\n",
      "step 685: loss=4.020439624786377\n",
      "step 686: loss=3.9562582969665527\n",
      "step 687: loss=3.857187509536743\n",
      "step 688: loss=3.9379940032958984\n",
      "step 689: loss=4.030976295471191\n",
      "step 690: loss=4.080989837646484\n",
      "step 691: loss=3.94681715965271\n",
      "step 692: loss=4.019579887390137\n",
      "step 693: loss=3.9735379219055176\n",
      "step 694: loss=4.019238471984863\n",
      "step 695: loss=4.019179344177246\n",
      "step 696: loss=3.9712741374969482\n",
      "step 697: loss=4.026883602142334\n",
      "step 698: loss=4.021551609039307\n",
      "step 699: loss=4.090793132781982\n",
      "step 700: loss=3.9863951206207275\n",
      "step 701: loss=4.023050308227539\n",
      "step 702: loss=3.958601474761963\n",
      "step 703: loss=3.948957681655884\n",
      "step 704: loss=3.9391560554504395\n",
      "step 705: loss=3.979315996170044\n",
      "step 706: loss=4.014042854309082\n",
      "step 707: loss=3.987943410873413\n",
      "step 708: loss=4.037786483764648\n",
      "step 709: loss=3.9132330417633057\n",
      "step 710: loss=3.93660569190979\n",
      "step 711: loss=3.9861817359924316\n",
      "step 712: loss=3.9920737743377686\n",
      "step 713: loss=3.935559034347534\n",
      "step 714: loss=4.01882791519165\n",
      "step 715: loss=3.9929792881011963\n",
      "step 716: loss=3.959758758544922\n",
      "step 717: loss=3.9396843910217285\n",
      "step 718: loss=4.044256687164307\n",
      "step 719: loss=4.004631996154785\n",
      "step 720: loss=3.996853828430176\n",
      "step 721: loss=4.005602836608887\n",
      "step 722: loss=4.0114874839782715\n",
      "step 723: loss=3.909653663635254\n",
      "step 724: loss=3.9997363090515137\n",
      "step 725: loss=4.052602291107178\n",
      "step 726: loss=3.982553720474243\n",
      "step 727: loss=3.9164369106292725\n",
      "step 728: loss=3.942601203918457\n",
      "step 729: loss=3.9261810779571533\n",
      "step 730: loss=3.895392894744873\n",
      "step 731: loss=3.9324474334716797\n",
      "step 732: loss=3.9205896854400635\n",
      "step 733: loss=3.862194061279297\n",
      "step 734: loss=3.914445400238037\n",
      "step 735: loss=3.8829867839813232\n",
      "step 736: loss=3.958883047103882\n",
      "step 737: loss=3.985666036605835\n",
      "step 738: loss=3.899744987487793\n",
      "step 739: loss=3.9055404663085938\n",
      "step 740: loss=3.9618642330169678\n",
      "step 741: loss=3.889052391052246\n",
      "step 742: loss=3.9750149250030518\n",
      "step 743: loss=3.955091953277588\n",
      "step 744: loss=3.9291818141937256\n",
      "step 745: loss=4.087095737457275\n",
      "step 746: loss=3.9071226119995117\n",
      "step 747: loss=3.909717082977295\n",
      "step 748: loss=3.9263739585876465\n",
      "step 749: loss=3.936264991760254\n",
      "step 750: loss=3.9365665912628174\n",
      "step 751: loss=3.938066005706787\n",
      "step 752: loss=3.970029830932617\n",
      "step 753: loss=3.9482243061065674\n",
      "step 754: loss=3.8840718269348145\n",
      "step 755: loss=3.952329635620117\n",
      "step 756: loss=3.8996760845184326\n",
      "step 757: loss=3.868910789489746\n",
      "step 758: loss=3.926823377609253\n",
      "step 759: loss=3.8954102993011475\n",
      "step 760: loss=4.0050368309021\n",
      "step 761: loss=3.949986696243286\n",
      "step 762: loss=3.946974277496338\n",
      "step 763: loss=3.8552606105804443\n",
      "step 764: loss=4.010585784912109\n",
      "step 765: loss=4.076684951782227\n",
      "step 766: loss=3.9837636947631836\n",
      "step 767: loss=3.980055332183838\n",
      "step 768: loss=4.009552478790283\n",
      "step 769: loss=3.981788158416748\n",
      "step 770: loss=3.9590961933135986\n",
      "step 771: loss=3.8400533199310303\n",
      "step 772: loss=3.88386607170105\n",
      "step 773: loss=3.8691532611846924\n",
      "step 774: loss=3.783965826034546\n",
      "step 775: loss=3.9049429893493652\n",
      "step 776: loss=3.9803521633148193\n",
      "step 777: loss=3.9815800189971924\n",
      "step 778: loss=3.8576600551605225\n",
      "step 779: loss=3.8969061374664307\n",
      "step 780: loss=3.9168310165405273\n",
      "step 781: loss=3.8810322284698486\n",
      "step 782: loss=3.8791961669921875\n",
      "step 783: loss=3.9240477085113525\n",
      "step 784: loss=3.853982448577881\n",
      "step 785: loss=3.9570164680480957\n",
      "step 786: loss=3.8988611698150635\n",
      "step 787: loss=3.906435251235962\n",
      "step 788: loss=3.8281397819519043\n",
      "step 789: loss=3.8748936653137207\n",
      "step 790: loss=3.7957098484039307\n",
      "step 791: loss=3.88936185836792\n",
      "step 792: loss=3.8909873962402344\n",
      "step 793: loss=3.9083378314971924\n",
      "step 794: loss=3.9673893451690674\n",
      "step 795: loss=3.7797911167144775\n",
      "step 796: loss=3.875593900680542\n",
      "step 797: loss=3.9253997802734375\n",
      "step 798: loss=3.9070682525634766\n",
      "step 799: loss=3.947261333465576\n",
      "step 800: loss=3.9517807960510254\n",
      "step 801: loss=3.9063897132873535\n",
      "step 802: loss=3.9117953777313232\n",
      "step 803: loss=3.870689630508423\n",
      "step 804: loss=3.8529112339019775\n",
      "step 805: loss=3.899077892303467\n",
      "step 806: loss=3.8730580806732178\n",
      "step 807: loss=3.8374412059783936\n",
      "step 808: loss=3.9627768993377686\n",
      "step 809: loss=3.8845608234405518\n",
      "step 810: loss=3.8707284927368164\n",
      "step 811: loss=3.812702178955078\n",
      "step 812: loss=3.9962551593780518\n",
      "step 813: loss=3.9128105640411377\n",
      "step 814: loss=3.903803825378418\n",
      "step 815: loss=3.901273012161255\n",
      "step 816: loss=3.8220882415771484\n",
      "step 817: loss=3.8226215839385986\n",
      "step 818: loss=3.8852691650390625\n",
      "step 819: loss=3.82008957862854\n",
      "step 820: loss=3.8163669109344482\n",
      "step 821: loss=3.7907116413116455\n",
      "step 822: loss=3.8253843784332275\n",
      "step 823: loss=3.8396363258361816\n",
      "step 824: loss=3.8457422256469727\n",
      "step 825: loss=3.8769731521606445\n",
      "step 826: loss=3.8439457416534424\n",
      "step 827: loss=3.876339912414551\n",
      "step 828: loss=3.908766746520996\n",
      "step 829: loss=3.8244807720184326\n",
      "step 830: loss=3.840985059738159\n",
      "step 831: loss=3.755624294281006\n",
      "step 832: loss=3.857574939727783\n",
      "step 833: loss=3.9703409671783447\n",
      "step 834: loss=3.698105812072754\n",
      "step 835: loss=3.7928614616394043\n",
      "step 836: loss=3.8883657455444336\n",
      "step 837: loss=3.9447708129882812\n",
      "step 838: loss=3.814781665802002\n",
      "step 839: loss=3.8951287269592285\n",
      "step 840: loss=3.802658796310425\n",
      "step 841: loss=3.8546926975250244\n",
      "step 842: loss=3.90655255317688\n",
      "step 843: loss=3.7954976558685303\n",
      "step 844: loss=3.80258846282959\n",
      "step 845: loss=3.848461389541626\n",
      "step 846: loss=3.8054921627044678\n",
      "step 847: loss=3.831833839416504\n",
      "step 848: loss=3.858421564102173\n",
      "step 849: loss=3.7204763889312744\n",
      "step 850: loss=3.9306366443634033\n",
      "step 851: loss=3.8283700942993164\n",
      "step 852: loss=3.828134775161743\n",
      "step 853: loss=3.854487895965576\n",
      "step 854: loss=3.8038671016693115\n",
      "step 855: loss=3.739840030670166\n",
      "step 856: loss=3.8414194583892822\n",
      "step 857: loss=3.840139627456665\n",
      "step 858: loss=3.858626365661621\n",
      "step 859: loss=3.8682193756103516\n",
      "step 860: loss=3.7725791931152344\n",
      "step 861: loss=3.7404379844665527\n",
      "step 862: loss=3.853250503540039\n",
      "step 863: loss=3.909426212310791\n",
      "step 864: loss=3.827120304107666\n",
      "step 865: loss=3.9142749309539795\n",
      "step 866: loss=3.8566269874572754\n",
      "step 867: loss=3.836930990219116\n",
      "step 868: loss=3.8507373332977295\n",
      "step 869: loss=3.7641050815582275\n",
      "step 870: loss=3.8466036319732666\n",
      "step 871: loss=3.8550658226013184\n",
      "step 872: loss=3.814547538757324\n",
      "step 873: loss=3.858736515045166\n",
      "step 874: loss=3.835547924041748\n",
      "step 875: loss=3.820634126663208\n",
      "step 876: loss=3.8446009159088135\n",
      "step 877: loss=3.8565011024475098\n",
      "step 878: loss=3.793048858642578\n",
      "step 879: loss=3.803140640258789\n",
      "step 880: loss=3.877279281616211\n",
      "step 881: loss=3.915501356124878\n",
      "step 882: loss=3.737924098968506\n",
      "step 883: loss=3.7830746173858643\n",
      "step 884: loss=3.773716926574707\n",
      "step 885: loss=3.769728660583496\n",
      "step 886: loss=3.76853084564209\n",
      "step 887: loss=3.774484395980835\n",
      "step 888: loss=3.908461093902588\n",
      "step 889: loss=3.785860300064087\n",
      "step 890: loss=3.840094804763794\n",
      "step 891: loss=3.9154365062713623\n",
      "step 892: loss=3.7070677280426025\n",
      "step 893: loss=3.876446485519409\n",
      "step 894: loss=3.8132681846618652\n",
      "step 895: loss=3.7787420749664307\n",
      "step 896: loss=3.808058023452759\n",
      "step 897: loss=3.7111711502075195\n",
      "step 898: loss=3.7674307823181152\n",
      "step 899: loss=3.886543035507202\n",
      "step 900: loss=3.837888717651367\n",
      "step 901: loss=3.7464873790740967\n",
      "step 902: loss=3.8434603214263916\n",
      "step 903: loss=3.8652989864349365\n",
      "step 904: loss=3.9370501041412354\n",
      "step 905: loss=3.7554266452789307\n",
      "step 906: loss=3.80645751953125\n",
      "step 907: loss=3.7218282222747803\n",
      "step 908: loss=3.841688394546509\n",
      "step 909: loss=3.838592529296875\n",
      "step 910: loss=3.7659902572631836\n",
      "step 911: loss=3.8026273250579834\n",
      "step 912: loss=3.730456829071045\n",
      "step 913: loss=3.7594165802001953\n",
      "step 914: loss=3.868187189102173\n",
      "step 915: loss=3.8208365440368652\n",
      "step 916: loss=3.8494954109191895\n",
      "step 917: loss=3.6943864822387695\n",
      "step 918: loss=3.825169324874878\n",
      "step 919: loss=3.802319049835205\n",
      "step 920: loss=3.7781949043273926\n",
      "step 921: loss=3.8065850734710693\n",
      "step 922: loss=3.795403003692627\n",
      "step 923: loss=3.826564311981201\n",
      "step 924: loss=3.7271037101745605\n",
      "step 925: loss=3.769099473953247\n",
      "step 926: loss=3.814549684524536\n",
      "step 927: loss=3.7501776218414307\n",
      "step 928: loss=3.8552398681640625\n",
      "step 929: loss=3.7635231018066406\n",
      "step 930: loss=3.757671594619751\n",
      "step 931: loss=3.7829813957214355\n",
      "step 932: loss=3.856001377105713\n",
      "step 933: loss=3.786350965499878\n",
      "step 934: loss=3.7992660999298096\n",
      "step 935: loss=3.853888511657715\n",
      "step 936: loss=3.7279489040374756\n",
      "step 937: loss=3.77148699760437\n",
      "step 938: loss=3.6774191856384277\n",
      "step 939: loss=3.7062580585479736\n",
      "step 940: loss=3.8540217876434326\n",
      "step 941: loss=3.714585065841675\n",
      "step 942: loss=3.75681209564209\n",
      "step 943: loss=3.8052268028259277\n",
      "step 944: loss=3.750840425491333\n",
      "step 945: loss=3.7266221046447754\n",
      "step 946: loss=3.8073298931121826\n",
      "step 947: loss=3.621952772140503\n",
      "step 948: loss=3.7910683155059814\n",
      "step 949: loss=3.783512830734253\n",
      "step 950: loss=3.916217088699341\n",
      "step 951: loss=3.8640291690826416\n",
      "step 952: loss=3.75536847114563\n",
      "step 953: loss=3.7300283908843994\n",
      "step 954: loss=3.797163963317871\n",
      "step 955: loss=3.7928595542907715\n",
      "step 956: loss=3.7810394763946533\n",
      "step 957: loss=3.8614728450775146\n",
      "step 958: loss=3.801198720932007\n",
      "step 959: loss=3.831483840942383\n",
      "step 960: loss=3.7052838802337646\n",
      "step 961: loss=3.786813735961914\n",
      "step 962: loss=3.731647253036499\n",
      "step 963: loss=3.7757680416107178\n",
      "step 964: loss=3.7474052906036377\n",
      "step 965: loss=3.761415481567383\n",
      "step 966: loss=3.806239128112793\n",
      "step 967: loss=3.7878215312957764\n",
      "step 968: loss=3.8432199954986572\n",
      "step 969: loss=3.7990047931671143\n",
      "step 970: loss=3.7246756553649902\n",
      "step 971: loss=3.711846113204956\n",
      "step 972: loss=3.7594316005706787\n",
      "step 973: loss=3.83084774017334\n",
      "step 974: loss=3.6742987632751465\n",
      "step 975: loss=3.684211015701294\n",
      "step 976: loss=3.766167163848877\n",
      "step 977: loss=3.725360155105591\n",
      "step 978: loss=3.788503408432007\n",
      "step 979: loss=3.7539544105529785\n",
      "step 980: loss=3.74045467376709\n",
      "step 981: loss=3.6661877632141113\n",
      "step 982: loss=3.8223159313201904\n",
      "step 983: loss=3.707447052001953\n",
      "step 984: loss=3.723768949508667\n",
      "step 985: loss=3.678398609161377\n",
      "step 986: loss=3.639932155609131\n",
      "step 987: loss=3.683429718017578\n",
      "step 988: loss=3.703873634338379\n",
      "step 989: loss=3.670771360397339\n",
      "step 990: loss=3.747714042663574\n",
      "step 991: loss=3.8158376216888428\n",
      "step 992: loss=3.765026569366455\n",
      "step 993: loss=3.719977617263794\n",
      "step 994: loss=3.709388017654419\n",
      "step 995: loss=3.685523271560669\n",
      "step 996: loss=3.697873115539551\n",
      "step 997: loss=3.727125883102417\n",
      "step 998: loss=3.804948091506958\n",
      "step 999: loss=3.704137086868286\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(1000):\n",
    "    \n",
    "    xb, yb = get_batch('train')\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    logits, loss = m(xb, yb)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"step {steps}: loss={loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d90e706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wh;;Sq.f ustNzknc\n",
      "kwgOj$dhPWr,SV?hsusiKpgXXUh;Apmem d?hESXI.i;TrJgkiF-oKbXCAA -botrngFCHAUQkn$\n",
      "\n",
      "pn$w-gHoi?wtd!\n",
      "LLULIfSK'bAw :M.ZtOptXEQcL?hfaofqbPd?OnonQQJMap$aypupIBYGUsZaI'ottllo..k$W$Akp?yl?ajKlzY!lx&QQLW? t,bXFkyhl-dmVsHeckhRl,jSClgjuk:3Iv\n",
      "?OqlrV;!Plxfzgy;;\n",
      "'mRjuBQ&xk!$\n",
      "h\n",
      "SiruDJgKuDny,S$ERf.?GSV\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e2c0a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "724c07fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1]\n",
    "        xbow[b,t] = torch.mean(xprev, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8685973e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(dim=1, keepdim=True)\n",
    "wei\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c31625c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbdff502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28f821b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "=========\n",
      "tensor([[5., 7.],\n",
      "        [2., 0.],\n",
      "        [5., 3.]])\n",
      "=========\n",
      "tensor([[5.0000, 7.0000],\n",
      "        [3.5000, 3.5000],\n",
      "        [4.0000, 3.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / torch.sum(a, dim=1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a@b\n",
    "print('=========')\n",
    "print(a)\n",
    "print('=========')\n",
    "print(b)\n",
    "print('=========')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db7eb657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "876ebc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow2, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "881e1828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
    "\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "# out = wei @ x\n",
    "\n",
    "v= value(x)\n",
    "out = wei @ v # (B, T, T) @ (B, T, 16) -> (B, T, 16)\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "102eea59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb75b24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5  # 这里乘以 head_size 的负 1/2 次方，是为了进行缩放（\"Scaled Dot-Product Attention\"），防止随着维度变大，点积值过大导致 softmax 梯度过小（变平），有助于保持数值稳定性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6de652db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 4, 6, 8])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.arange(0, 10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48aca28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 0.7943, 0.6310, 0.5012, 0.3981, 0.3162, 0.2512, 0.1995, 0.1585,\n",
      "        0.1259, 0.1000, 0.0794, 0.0631, 0.0501, 0.0398, 0.0316, 0.0251, 0.0200,\n",
      "        0.0158, 0.0126])\n"
     ]
    }
   ],
   "source": [
    "# 也就是数学公式：base^(-2i/d)\n",
    "inv_freq = 1.0 / (100 ** (torch.arange(0, 40, 2).float() / 40))\n",
    "print(inv_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33d7fbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [2, 4, 6],\n",
      "        [3, 6, 9]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([1,2,3])\n",
    "inv_freq = torch.tensor([1,2,3])\n",
    "\n",
    "freqs = torch.einsum('i,j->ij', t, inv_freq)\n",
    "print(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f1e4778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2])\n",
      "tensor([3, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1,2,3,4])\n",
    "print(x[..., :2])\n",
    "print(x[..., 2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88eecc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "tril = torch.tril(torch.ones(4, 4))\n",
    "x = torch.zeros(4, 4)\n",
    "x.masked_fill_(tril == 0, float('-inf'))\n",
    "x = F.softmax(x, dim=-1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15384e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "测试 1: 基础的 mask 索引\n",
      "==================================================\n",
      "初始 output (形状 torch.Size([4, 3])):\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "mask: tensor([ True, False,  True, False])\n",
      "mask.sum() = 2 (有几个 True)\n",
      "n = 2\n",
      "\n",
      "expert_output (形状 torch.Size([2, 3])):\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "\n",
      "expert_weights (形状 torch.Size([2, 1])):\n",
      "tensor([[0.8000],\n",
      "        [0.6000]])\n",
      "\n",
      "更新后的 output (形状 torch.Size([4, 3])):\n",
      "tensor([[0.8000, 1.6000, 2.4000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [2.4000, 3.0000, 3.6000],\n",
      "        [0.0000, 0.0000, 0.0000]])\n",
      "\n",
      "注意: 只有 token 0 和 token 2 被更新了！\n",
      "\n",
      "==================================================\n",
      "测试 2: 模拟多个专家累加\n",
      "==================================================\n",
      "专家 A 选中 token 0, 2，输出=10，权重=[0.5, 0.5]\n",
      "专家 B 选中 token 0, 3，输出=100，权重=[0.5, 0.8]\n",
      "\n",
      "专家 A 处理后:\n",
      "tensor([[5., 5., 5.],\n",
      "        [0., 0., 0.],\n",
      "        [5., 5., 5.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "专家 B 处理后 (累加):\n",
      "tensor([[55., 55., 55.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 5.,  5.,  5.],\n",
      "        [80., 80., 80.]])\n",
      "\n",
      "分析:\n",
      "- Token 0: 被 A 和 B 都选中 -> 0.5*10 + 0.5*100 = 55\n",
      "- Token 1: 没人选 -> 0\n",
      "- Token 2: 只被 A 选中 -> 0.5*10 = 5\n",
      "- Token 3: 只被 B 选中 -> 0.8*100 = 80\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "测试 PyTorch 中 mask 布尔索引的写法\n",
    "演示 output[mask] += ... 的行为\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"测试 1: 基础的 mask 索引\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 假设有 4 个 token (B*T = 4)，嵌入维度 C = 3\n",
    "B_T = 4\n",
    "C = 3\n",
    "\n",
    "# 初始化 output 为全 0\n",
    "output = torch.zeros(B_T, C)\n",
    "print(f\"初始 output (形状 {output.shape}):\")\n",
    "print(output)\n",
    "\n",
    "# mask: 标记哪些 token 选中了当前专家\n",
    "# 假设 token 0 和 token 2 选中了当前专家\n",
    "mask = torch.tensor([True, False, True, False])\n",
    "print(f\"\\nmask: {mask}\")\n",
    "print(f\"mask.sum() = {mask.sum()} (有几个 True)\")\n",
    "\n",
    "# n = 选中专家的 token 数量\n",
    "n = mask.sum().item()\n",
    "print(f\"n = {n}\")\n",
    "\n",
    "# 专家计算结果 (只有 n 个)\n",
    "expert_output = torch.tensor([\n",
    "    [1.0, 2.0, 3.0],   # token 0 的专家输出\n",
    "    [4.0, 5.0, 6.0],   # token 2 的专家输出\n",
    "])\n",
    "print(f\"\\nexpert_output (形状 {expert_output.shape}):\")\n",
    "print(expert_output)\n",
    "\n",
    "# 权重 (只有 n 个)\n",
    "expert_weights = torch.tensor([[0.8], [0.6]])  # 形状 (n, 1)\n",
    "print(f\"\\nexpert_weights (形状 {expert_weights.shape}):\")\n",
    "print(expert_weights)\n",
    "\n",
    "# 关键操作：只更新 mask 为 True 的位置\n",
    "output[mask] += expert_weights * expert_output\n",
    "\n",
    "print(f\"\\n更新后的 output (形状 {output.shape}):\")\n",
    "print(output)\n",
    "print(\"\\n注意: 只有 token 0 和 token 2 被更新了！\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"测试 2: 模拟多个专家累加\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 重新初始化\n",
    "output = torch.zeros(B_T, C)\n",
    "\n",
    "# 专家 A: token 0, 2 选中\n",
    "mask_A = torch.tensor([True, False, True, False])\n",
    "expert_A_output = torch.ones(2, C) * 10  # 专家 A 输出全是 10\n",
    "weights_A = torch.tensor([[0.5], [0.5]])\n",
    "\n",
    "# 专家 B: token 0, 3 选中\n",
    "mask_B = torch.tensor([True, False, False, True])\n",
    "expert_B_output = torch.ones(2, C) * 100  # 专家 B 输出全是 100\n",
    "weights_B = torch.tensor([[0.5], [0.8]])\n",
    "\n",
    "print(\"专家 A 选中 token 0, 2，输出=10，权重=[0.5, 0.5]\")\n",
    "print(\"专家 B 选中 token 0, 3，输出=100，权重=[0.5, 0.8]\")\n",
    "\n",
    "# 模拟循环中的累加\n",
    "output[mask_A] += weights_A * expert_A_output\n",
    "print(f\"\\n专家 A 处理后:\")\n",
    "print(output)\n",
    "\n",
    "output[mask_B] += weights_B * expert_B_output\n",
    "print(f\"\\n专家 B 处理后 (累加):\")\n",
    "print(output)\n",
    "\n",
    "print(\"\\n分析:\")\n",
    "print(\"- Token 0: 被 A 和 B 都选中 -> 0.5*10 + 0.5*100 = 55\")\n",
    "print(\"- Token 1: 没人选 -> 0\")\n",
    "print(\"- Token 2: 只被 A 选中 -> 0.5*10 = 5\")\n",
    "print(\"- Token 3: 只被 B 选中 -> 0.8*100 = 80\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff75aa20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
